{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder using HuggingFace datasets and tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import (\n",
    "    lax,\n",
    "    random,\n",
    "    nn,\n",
    "    numpy as jnp\n",
    ")\n",
    "import optax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax import Module, Parameter\n",
    "from mlax.nn import (\n",
    "    Embed,\n",
    "    Linear,\n",
    "    Bias,\n",
    "    Series\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import EncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/zongyf02/projects/mlax/examples/Encoder/../data/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/home/zongyf02/projects/mlax/examples/Encoder/../data/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "imdb_train = load_dataset(\"imdb\", cache_dir=\"../data\", split=\"train\")\n",
    "imdb_test = load_dataset(\"imdb\", cache_dir=\"../data\", split=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize datasets using a pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512\n",
    "tokenizer = Tokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer.enable_truncation(seq_len)\n",
    "tokenizer.enable_padding(length=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(batch):\n",
    "    encodings = tokenizer.encode_batch(batch[\"text\"])\n",
    "    batch[\"ids\"] = [encoding.ids for encoding in encodings]\n",
    "    batch[\"mask\"] = [\n",
    "        [bool(i) for i in encoding.attention_mask] for encoding in encodings\n",
    "    ]\n",
    "    del batch[\"text\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/zongyf02/projects/mlax/examples/data/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-4d102de10fe8f91d.arrow\n",
      "Loading cached processed dataset at /home/zongyf02/projects/mlax/examples/data/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-6357fc30ae0156bd.arrow\n"
     ]
    }
   ],
   "source": [
    "imdb_train_tokenized = imdb_train.map(\n",
    "    tokenization, batched=True, batch_size=None\n",
    ")\n",
    "imdb_test_tokenized = imdb_test.map(\n",
    "    tokenization, batched=True, batch_size=None\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_tokenized.set_format(type=\"numpy\")\n",
    "imdb_test_tokenized.set_format(type=\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 196\n"
     ]
    }
   ],
   "source": [
    "def numpy_collate(batch):\n",
    "  if isinstance(batch[0], np.ndarray):\n",
    "    return np.stack(batch)\n",
    "  elif isinstance(batch[0], (tuple,list)):\n",
    "    transposed = zip(*batch)\n",
    "    return [numpy_collate(samples) for samples in transposed]\n",
    "  elif isinstance(batch[0], dict):\n",
    "    res = {}\n",
    "    for key in batch[0]:\n",
    "      res[key] = numpy_collate([d[key] for d in batch])\n",
    "    return res\n",
    "  else:\n",
    "    return np.array(batch)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(\n",
    "    imdb_train_tokenized, batch_size, shuffle=True, collate_fn=numpy_collate, num_workers=0\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    imdb_test_tokenized, batch_size, collate_fn=numpy_collate, num_workers=0\n",
    ")\n",
    "print(len(train_dataloader), len(test_dataloader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.5083568\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "class RotaryEmbed(Module):\n",
    "    def __init__(self, seq_len, embed_dim):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (\n",
    "            10000.0 ** (jnp.arange(0, embed_dim, 2, dtype=jnp.float32) / embed_dim)\n",
    "        )\n",
    "        pos = jnp.arange(seq_len, dtype=jnp.float32)\n",
    "        pos_enc = lax.dot_general(pos, inv_freq, (((), ()), ((), ())))\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.sin_enc = Parameter(trainable=False, data=lax.sin(pos_enc))\n",
    "        self.cos_enc = Parameter(trainable=False, data=lax.cos(pos_enc))\n",
    "    \n",
    "    def init(self, x):\n",
    "        pass\n",
    "\n",
    "    def apply(self, x, rng=None, inference_mode=False, batch_axis_name=()):\n",
    "        shape = self.sin_enc.data.shape[:-1] + (-1,)\n",
    "        sin = jnp.stack(\n",
    "            [self.sin_enc.data, self.sin_enc.data], axis=-1\n",
    "        ).reshape(shape)\n",
    "        cos = jnp.stack(\n",
    "            [self.cos_enc.data, self.cos_enc.data], axis=-1\n",
    "        ).reshape(shape)\n",
    "        rotated_x = jnp.stack(\n",
    "            [-x[..., 1::2], x[..., ::2]], axis=-1\n",
    "        ).reshape(x.shape)\n",
    "        return x * cos + rotated_x * sin\n",
    "\n",
    "class Model(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rng,\n",
    "        vocab_size,\n",
    "        seq_len,\n",
    "        embed_dim=256,\n",
    "        num_heads=8,\n",
    "        ff_depth=1024,\n",
    "        act_fn=nn.gelu,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        keys_iter = iter([random.fold_in(rng, i) for i in range(5)])\n",
    "\n",
    "        self.embed = Embed(\n",
    "            next(keys_iter), vocab_size, embed_dim\n",
    "        )\n",
    "        self.rotary = RotaryEmbed(seq_len, embed_dim)\n",
    "\n",
    "        self.encoder1 = EncoderBlock(\n",
    "            next(keys_iter), num_heads, ff_depth, act_fn, dropout\n",
    "        )\n",
    "        self.encoder2 = EncoderBlock(\n",
    "            next(keys_iter), num_heads, ff_depth, act_fn, dropout\n",
    "        )\n",
    "\n",
    "        self.fc = Series([\n",
    "            Linear(next(keys_iter), 1),\n",
    "            Bias(next(keys_iter), -1)\n",
    "        ])\n",
    "    \n",
    "    def init(self, x):\n",
    "        pass\n",
    "\n",
    "    def apply(self, xm, rng, inference_mode=False, batch_axis_name=()):\n",
    "        ids, mask = xm\n",
    "        embeddings, self.embed = self.embed(\n",
    "            ids, None, inference_mode, batch_axis_name\n",
    "        )\n",
    "        embeddings, self.rotary = self.rotary(\n",
    "            embeddings, None, inference_mode, batch_axis_name\n",
    "        )\n",
    "        activations, self.encoder1 = self.encoder1(\n",
    "            (embeddings, mask), random.fold_in(rng, 0),\n",
    "            inference_mode, batch_axis_name\n",
    "        )\n",
    "        activations, self.rotary = self.rotary(\n",
    "            activations, None, inference_mode, batch_axis_name\n",
    "        )\n",
    "        activations, self.encoder2 = self.encoder2(\n",
    "            (activations, mask), random.fold_in(rng, 1),\n",
    "            inference_mode, batch_axis_name\n",
    "        )\n",
    "        activations = jnp.reshape(activations, (-1,))\n",
    "        activations, self.fc = self.fc(\n",
    "            activations, None, inference_mode, batch_axis_name\n",
    "        )\n",
    "        return jnp.squeeze(activations)\n",
    "\n",
    "rng1 = random.PRNGKey(0)\n",
    "rng1, rng2 = random.fold_in(rng1, 0), random.fold_in(rng1, 1)\n",
    "model = Model(rng1, tokenizer.get_vocab_size(), seq_len, dropout=0.2)\n",
    "\n",
    "# Induce lazy initialization\n",
    "for batch in train_dataloader:\n",
    "    ids, mask = batch[\"ids\"], batch[\"mask\"]\n",
    "    activations, _ = model((ids[0], mask[0]), rng2, inference_mode=True)\n",
    "    print(activations)\n",
    "    print(activations.dtype)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(batched_preds, batched_targets):\n",
    "    return optax.sigmoid_binary_cross_entropy(\n",
    "        batched_preds, batched_targets\n",
    "    ).mean() # Optax returns per-example loss, this returns the mean batch loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer using Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adamw(5e-4, weight_decay=1e-2)\n",
    "optim_state = optimizer.init(model.filter())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(X, y, rng, model, optim_state):\n",
    "    def _model_loss(X, y, rng, trainables, non_trainables):\n",
    "        model = trainables.combine(non_trainables)\n",
    "        preds, model = jax.vmap(\n",
    "            model.__call__,\n",
    "            in_axes = (0, None, None, None),\n",
    "            out_axes = (0, None),\n",
    "            axis_name = \"N\"\n",
    "        )(X, rng, False, \"N\")\n",
    "        return loss_fn(preds, y), model\n",
    "\n",
    "    # Find batch loss and gradients with resect to trainables\n",
    "    trainables, non_trainables = model.partition()\n",
    "    (loss, model), gradients = jax.value_and_grad(\n",
    "        _model_loss,\n",
    "        argnums=3, # gradients wrt trainables (argument 2 of model_training_loss)\n",
    "        has_aux=True # model is auxiliary data, loss is the true ouput\n",
    "    )(X, y, rng, trainables, non_trainables)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    trainables, non_trainables = model.partition()\n",
    "    gradients, optim_state = optimizer.update(\n",
    "        gradients, optim_state, trainables\n",
    "    )\n",
    "\n",
    "    # Update parameters with new gradients\n",
    "    trainables = optax.apply_updates(gradients, trainables)\n",
    "    return loss, trainables.combine(non_trainables), optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def test_step(X, y, rng, model):\n",
    "    preds, _ = jax.vmap(\n",
    "        model.__call__,\n",
    "        in_axes = (0, None, None, None),\n",
    "        out_axes = (0, None),\n",
    "        axis_name = \"N\"\n",
    "    )(X, rng, True, \"N\")\n",
    "    accurate = (jnp.round(nn.sigmoid(preds)) == y).astype(jnp.int32).sum()\n",
    "    return loss_fn(preds, y), accurate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, rng, model, optim_state):\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        _rng = random.fold_in(rng, i)\n",
    "        ids, mask, y = batch[\"ids\"], batch[\"mask\"], batch[\"label\"]\n",
    "        loss, model, optim_state = train_step(\n",
    "            (ids, mask), y, _rng, model, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / len(dataloader)}\") \n",
    "    return model, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, rng, model):\n",
    "    test_loss, accurate = 0.0, 0\n",
    "    for batch in dataloader:\n",
    "        ids, mask, y = batch[\"ids\"], batch[\"mask\"], batch[\"label\"]\n",
    "        loss, acc = test_step((ids, mask), y, rng, model)\n",
    "        test_loss += loss\n",
    "        accurate += acc\n",
    "\n",
    "    print(f\"Test loss: {test_loss / len(dataloader)}, accuracy: {accurate / len(dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    rng,\n",
    "    model,\n",
    "    optim_state,\n",
    "    epochs,\n",
    "    test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        _rng = random.fold_in(rng, i)\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        model, optim_state = train_epoch(\n",
    "            train_dataloader, _rng, model, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(test_dataloader, _rng, model)\n",
    "        print(f\"----------------\")\n",
    "    return model, optim_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Encoder on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 1.1788954734802246\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.10283064842224121\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.01246965117752552\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.0016183697152882814\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.0004987806896679103\n",
      "Test loss: 0.7959541082382202, accuracy: 0.8016799688339233\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.00029921604436822236\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.00021634899894706905\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.0001659040863160044\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.0001315609406447038\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.00010687522444641218\n",
      "Test loss: 0.8817386627197266, accuracy: 0.8104400038719177\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 8.837754285195842e-05\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 7.371755054919049e-05\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 6.195362220751122e-05\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 5.2695504564326257e-05\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 4.523693132796325e-05\n",
      "Test loss: 0.962737500667572, accuracy: 0.8130799531936646\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 3.884676698362455e-05\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 3.347613528603688e-05\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 2.909899194492027e-05\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 2.528155891923234e-05\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 2.194454827986192e-05\n",
      "Test loss: 1.040713906288147, accuracy: 0.8161199688911438\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 1.9109704226139e-05\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 1.671680001891218e-05\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 1.4585482858819887e-05\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 1.2789219908881932e-05\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 1.1159992027387489e-05\n",
      "Test loss: 1.1083701848983765, accuracy: 0.8189199566841125\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 9.777862032933626e-06\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 8.540166163584217e-06\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 7.51065772419679e-06\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 6.597086667170515e-06\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 5.779112598247593e-06\n",
      "Test loss: 1.1905516386032104, accuracy: 0.8209999799728394\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "with jax.default_matmul_precision(\"float32\"):\n",
    "    new_params, new_optim_state = train_loop(\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        rng2,\n",
    "        model,\n",
    "        optim_state,\n",
    "        30, 5\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c3d7272c1eba356ec9149ec42daf5acdf55d6fdb447aefce6509807a5e73802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
