{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder using HuggingFace datasets and tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import (\n",
    "    random,\n",
    "    nn,\n",
    "    numpy as jnp\n",
    ")\n",
    "from functools import partial\n",
    "import optax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax import Module, is_trainable\n",
    "from mlax.nn import (\n",
    "    Embed,\n",
    "    Linear,\n",
    "    Bias,\n",
    "    Series\n",
    ")\n",
    "from mlax.nn.functional import dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import EncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/zongyf02/projects/mlax/examples/Encoder/../data/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/home/zongyf02/projects/mlax/examples/Encoder/../data/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "imdb_train = load_dataset(\"imdb\", cache_dir=\"../data\", split=\"train\")\n",
    "imdb_test = load_dataset(\"imdb\", cache_dir=\"../data\", split=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize datasets using a pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer.enable_truncation(512)\n",
    "tokenizer.enable_padding(length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(batch):\n",
    "    encodings = tokenizer.encode_batch(batch[\"text\"])\n",
    "    batch[\"ids\"] = [encoding.ids for encoding in encodings]\n",
    "    batch[\"mask\"] = [encoding.attention_mask for encoding in encodings]\n",
    "    del batch[\"text\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99d9cc30aa841219582b0ae21c8e638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2469f551571f4811a730359b6f98c5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_train_tokenized = imdb_train.map(\n",
    "    tokenization, batched=True, batch_size=None\n",
    ")\n",
    "imdb_test_tokenized = imdb_test.map(\n",
    "    tokenization, batched=True, batch_size=None\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_tokenized.set_format(type=\"numpy\")\n",
    "imdb_test_tokenized.set_format(type=\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 196\n"
     ]
    }
   ],
   "source": [
    "def numpy_collate(batch):\n",
    "  if isinstance(batch[0], np.ndarray):\n",
    "    return np.stack(batch)\n",
    "  elif isinstance(batch[0], (tuple,list)):\n",
    "    transposed = zip(*batch)\n",
    "    return [numpy_collate(samples) for samples in transposed]\n",
    "  elif isinstance(batch[0], dict):\n",
    "    res = {}\n",
    "    for key in batch[0]:\n",
    "      res[key] = numpy_collate([d[key] for d in batch])\n",
    "    return res\n",
    "  else:\n",
    "    return np.array(batch)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(\n",
    "    imdb_train_tokenized, batch_size, shuffle=True, collate_fn=numpy_collate, num_workers=0\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    imdb_test_tokenized, batch_size, collate_fn=numpy_collate, num_workers=0\n",
    ")\n",
    "print(len(train_dataloader), len(test_dataloader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification model with learnable positional embedding and 2 encoders\n",
    "class Model(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rng,\n",
    "        vocab_size,\n",
    "        seq_len,\n",
    "        feature_embed_dim = 248,\n",
    "        pos_embed_dim = 8,\n",
    "        num_heads = 8,\n",
    "        ff_depth = 1024,\n",
    "        act_fn=nn.gelu,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        rngs_iter = iter(random.split(rng, 6))\n",
    "\n",
    "        self.feature_embeddings = Embed(\n",
    "            next(rngs_iter), vocab_size, feature_embed_dim\n",
    "        )\n",
    "        self.pos_embeddings = Embed(\n",
    "            next(rngs_iter), seq_len, pos_embed_dim\n",
    "        )\n",
    "\n",
    "        model_depth = (feature_embed_dim + pos_embed_dim)\n",
    "        self.encoder1 = EncoderBlock(\n",
    "            next(rngs_iter), model_depth, num_heads, ff_depth, act_fn, dropout\n",
    "        )\n",
    "        self.encoder2 = EncoderBlock(\n",
    "            next(rngs_iter), model_depth, num_heads, ff_depth, act_fn, dropout\n",
    "        )\n",
    "\n",
    "        self.fc = Series([\n",
    "            Linear(next(rngs_iter), 1),\n",
    "            Bias(next(rngs_iter), -1)\n",
    "        ])\n",
    "\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    @partial(\n",
    "        jax.vmap,\n",
    "        in_axes = (None, 0, None, None),\n",
    "        out_axes = (0, None),\n",
    "        axis_name = \"batch\"\n",
    "    ) # Add leading batch dimension\n",
    "    def __call__(self, x, rng, inference_mode=False):\n",
    "        ids, mask = x\n",
    "        rng1, rng2, rng3 = random.split(rng, 3)\n",
    "\n",
    "        # Feature and positional embeddings are appended\n",
    "        embeddings, self.feature_embeddings = self.feature_embeddings(\n",
    "            ids, None, inference_mode\n",
    "        )\n",
    "        pos_embeddings, self.pos_embeddings = self.pos_embeddings(\n",
    "            jnp.arange((len(ids))), None, inference_mode\n",
    "        )\n",
    "        embeddings = jnp.append(\n",
    "            embeddings, pos_embeddings, axis=1\n",
    "        )\n",
    "        if not inference_mode:\n",
    "            embeddings = dropout(embeddings, rng1, self.dropout)\n",
    "\n",
    "        # Encoders\n",
    "        activations, self.encoder1 = self.encoder1(\n",
    "            (embeddings, mask),\n",
    "            rng2,\n",
    "            inference_mode\n",
    "        )\n",
    "        activations, self.encoder2 = self.encoder2(\n",
    "            (activations, mask),\n",
    "            rng3,\n",
    "            inference_mode\n",
    "        )\n",
    "\n",
    "        # Dense layer\n",
    "        activations = jnp.reshape(activations, (-1,))\n",
    "        activations, self.fc = self.fc(activations, None, inference_mode)\n",
    "        return activations, self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1)\n"
     ]
    }
   ],
   "source": [
    "rng1, rng2 = random.split(random.PRNGKey(0))\n",
    "model = Model(rng1, tokenizer.get_vocab_size(), 512)\n",
    "\n",
    "# Induce lazy weight initialization\n",
    "for batch in train_dataloader:\n",
    "    acts, model = model((batch[\"ids\"], batch[\"mask\"]), rng2, False)\n",
    "    print(acts.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    preds: jnp.array,\n",
    "    targets: np.array\n",
    "):\n",
    "    return optax.sigmoid_binary_cross_entropy(\n",
    "        preds,\n",
    "        targets\n",
    "    ).mean() # Optax returns per-example loss, this returns the mean batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    rng: jax.Array,\n",
    "    trainables,\n",
    "    non_trainables\n",
    "):\n",
    "    model = trainables.combine(non_trainables)\n",
    "    preds, model = model(x_batch, rng, False)\n",
    "    return loss_fn(jnp.squeeze(preds), y_batch), model\n",
    "\n",
    "@jax.jit\n",
    "def model_inference_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    rng: jax.Array,\n",
    "    model: Module\n",
    "):\n",
    "    preds, _ = model(x_batch, rng, True)\n",
    "    preds = jnp.squeeze(preds)\n",
    "    return loss_fn(preds, y_batch), preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer using Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adamw(1e-4, weight_decay=1e-2)\n",
    "optim_state = optimizer.init(model.filter(is_trainable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(\n",
    "    x_batch: np.array, \n",
    "    y_batch: np.array,\n",
    "    rng: jax.Array,\n",
    "    model: Module,\n",
    "    optim_state\n",
    "):\n",
    "    # Find batch loss and gradients with repect to trainables\n",
    "    (loss, model), gradients = jax.value_and_grad(\n",
    "        model_training_loss,\n",
    "        argnums=3, # gradients wrt trainables (argument 3 of model_training_loss)\n",
    "        has_aux=True # model is auxiliary data, loss is the true ouput\n",
    "    )(x_batch, y_batch, rng, *model.partition())\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    trainables, non_trainables = model.partition()\n",
    "    gradients, optim_state = optimizer.update(gradients, optim_state, trainables)\n",
    "\n",
    "    # Update model_weights with new gradients\n",
    "    trainables = optax.apply_updates(gradients, trainables)\n",
    "    return loss, trainables.combine(non_trainables), optim_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    dataloader,\n",
    "    rng,\n",
    "    model,\n",
    "    optim_state\n",
    "):\n",
    "    train_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        x_batch = (batch[\"ids\"], batch[\"mask\"])\n",
    "        y_batch = batch[\"label\"]\n",
    "        sub_rng, rng = random.split(rng)\n",
    "        loss, model, optim_state = train_step(\n",
    "            x_batch, y_batch, sub_rng,\n",
    "            model,\n",
    "            optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / len(dataloader)}\") \n",
    "    return model, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    dataloader,\n",
    "    rng,\n",
    "    model\n",
    "):\n",
    "    test_loss, accuracy = 0.0, 0\n",
    "    for batch in dataloader:\n",
    "        x_batch = (batch[\"ids\"], batch[\"mask\"])\n",
    "        y_batch = batch[\"label\"]\n",
    "        sub_rng, rng = random.split(rng)\n",
    "        loss, preds = model_inference_loss(\n",
    "            x_batch, y_batch, sub_rng, model\n",
    "        )\n",
    "        test_loss += loss\n",
    "        accuracy += (nn.sigmoid(preds).round() == y_batch).sum()\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / len(dataloader)}, accuracy: {accuracy / len(dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    rng,\n",
    "    model,\n",
    "    optim_state,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        model, optim_state = train_epoch(\n",
    "            train_dataloader,\n",
    "            rng,\n",
    "            model,\n",
    "            optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(test_dataloader, rng, model)\n",
    "        print(f\"----------------\")\n",
    "\n",
    "    return model, optim_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Encoder on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 0.9046181440353394\n",
      "Test loss: 0.599520206451416, accuracy: 0.6882399916648865\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 1.960471272468567\n",
      "Test loss: 0.7225207686424255, accuracy: 0.7914800047874451\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.6404978036880493\n",
      "Test loss: 0.5163156390190125, accuracy: 0.8023999929428101\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.7001040577888489\n",
      "Test loss: 0.6478968262672424, accuracy: 0.80103999376297\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.7255018949508667\n",
      "Test loss: 0.5033421516418457, accuracy: 0.834119975566864\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.32198628783226013\n",
      "Test loss: 0.43705442547798157, accuracy: 0.8501200079917908\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.20895497500896454\n",
      "Test loss: 0.4127040207386017, accuracy: 0.8517199754714966\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.14484575390815735\n",
      "Test loss: 0.45383429527282715, accuracy: 0.8528800010681152\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.11223694682121277\n",
      "Test loss: 0.4405384361743927, accuracy: 0.8499599695205688\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.0920342430472374\n",
      "Test loss: 0.47910717129707336, accuracy: 0.8473199605941772\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_model, new_optim_state = train_loop(\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    rng2,\n",
    "    model,\n",
    "    optim_state,\n",
    "    10, 1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c3d7272c1eba356ec9149ec42daf5acdf55d6fdb447aefce6509807a5e73802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
