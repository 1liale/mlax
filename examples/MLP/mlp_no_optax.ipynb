{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation without Optax.\n",
    "Optax is a JAX optimization library by DeepMind. It is now the recommended way\n",
    "to update weights in mlax. See `mlp_optax.ipynb` for this same example but using\n",
    "Optax loss functions and optimizers.\n",
    "\n",
    "This notebook uses the now deprecated SGD optimizer available in mlax before\n",
    "prior to version 1.0.0.\n",
    "\n",
    "A Pytorch reference implementation is available at `mlp_reference.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import (\n",
    "    nn,\n",
    "    random,\n",
    "    tree_util\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import `Linear`, `Bias`, and `F` transformations from `mlax.nn` to build some\n",
    "dense layers.\n",
    "\n",
    "We import `Series` from `mlax.block` to stack the dense and dropout layers into\n",
    "an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax.nn import Linear, Bias, F\n",
    "from mlax.block import Series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import helpers to load data from Pytorch.\n",
    "\n",
    "We also import a categorical crossentropy loss function and a SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import batch, load_mnist\n",
    "from optim import (\n",
    "    sparse_categorical_crossentropy,\n",
    "    sgd_init,\n",
    "    sgd_step,\n",
    "    apply_updates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and batch the MNIST datasets.\n",
    "We use helper functions to load in Pytorch datasets as numpy and convert them in\n",
    "to lists containing the batches.\n",
    "\n",
    "Checkout\n",
    "[Training a Simple Neural Network with tensorflow/datasets Data Loading](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html) and\n",
    "[Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)\n",
    "for other ways to load in Tensorflow and Pytorch datsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n",
      "469 469\n",
      "79 79\n"
     ]
    }
   ],
   "source": [
    "# Load in datasets with helper\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(\"../data\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Batch datasets\n",
    "batch_size = 128\n",
    "X_train, y_train = batch(X_train, y_train, batch_size)\n",
    "X_test, y_test = batch(X_test, y_test, batch_size)\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLP model parameters.\n",
    "`model_init` consumes a `jax.random.PRNGKey` when initializing the parameters.\n",
    "Read more about random numbers in JAX [here](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html).\n",
    "\n",
    "`F` is a wrapper around a stateless function. `Linear` is a linear\n",
    "transformation without bias. `Bias` adds a bias term.\n",
    "\n",
    "Each `init` function returns a `trainables`, `non_trainables`, and\n",
    "`hyperparams`.\n",
    "\n",
    "`trainables` are trainable weights. `non_trainables` are non-trainable\n",
    "variables. `hyperparams` are additional parameters required by the forward pass.\n",
    "\n",
    "The `trainables` and `non_trainables` are PyTrees of JAX arrays. Read more about\n",
    "JAX PyTrees [here](https://jax.readthedocs.io/en/latest/pytrees.html).\n",
    "\n",
    "`hyperparams` is a NamedTuple containing Python types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(key):\n",
    "    keys_iter = iter(random.split(key, 6))\n",
    "    return Series.init(\n",
    "        # Convert int8 numpy inputs to float32 JAX arrays and flatten them\n",
    "        F.init(lambda x:jnp.reshape(\n",
    "            jnp.asarray(x, jnp.float32) / 255.0,\n",
    "            (len(x), -1))\n",
    "        ),\n",
    "\n",
    "        # Dense layer with relu activation\n",
    "        Linear.init(\n",
    "            key=next(keys_iter),\n",
    "            in_features=28 * 28,\n",
    "            out_features=512,\n",
    "        ),\n",
    "        Bias.init(\n",
    "            key=next(keys_iter),\n",
    "            in_feature_shape=(512,)\n",
    "        ),\n",
    "        F.init(nn.relu),\n",
    "\n",
    "        # Dense layer with relu activation\n",
    "        Linear.init(next(keys_iter), 512, 512),\n",
    "        Bias.init(next(keys_iter), (512,)),\n",
    "        F.init(nn.relu),\n",
    "        \n",
    "        # Dense layer with softmax\n",
    "        Linear.init(next(keys_iter), 512, 10),\n",
    "        Bias.init(next(keys_iter), (10,)),\n",
    "        F.init(nn.softmax),\n",
    "    )\n",
    "\n",
    "trainables, non_trainables, hyperparams = model_init(random.PRNGKey(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MLP dataflow.\n",
    "`Series.fwd` takes in batched input features and tuples of `trainables`,\n",
    "`non_trainables`, and `hyperparams`. It figures out which layer each\n",
    "`hyperparams` is for, and calls their forward pass functions on the input\n",
    "features in sequence.\n",
    "\n",
    "It returns the model predictions and updated `non_trainables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fwd = Series.fwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=sparse_categorical_crossentropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two convenience functions.\n",
    "\n",
    "``model_training_loss`` returns the batch loss and updated `non_trainables` from\n",
    "batched inputs and targets.\n",
    "\n",
    "``model_inference_preds_loss`` returns the predictions and batch loss from\n",
    "batched inputs and targets.\n",
    "\n",
    "We jit-compile the ``model_inference_preds_loss`` for significant speedups. Note\n",
    "that `hyperparams` is a static argument because it is made of Python types, not\n",
    "valid JAX types, and it also used interally for control flow. Read more about\n",
    "jit-compilation [here](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainig_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    trainables,\n",
    "    non_trainables,\n",
    "    hyperparams,\n",
    "):\n",
    "    preds, non_trainables = model_fwd(\n",
    "        x_batch, trainables, non_trainables, hyperparams\n",
    "    )\n",
    "    return loss_fn(preds, y_batch), non_trainables\n",
    "\n",
    "@tree_util.Partial(jax.jit, static_argnames=\"hyperparams\")\n",
    "def model_inference_preds_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    trainables,\n",
    "    non_trainables,\n",
    "    hyperparams\n",
    "):\n",
    "    preds, _ = model_fwd(\n",
    "        x_batch, trainables, non_trainables, hyperparams\n",
    "    )\n",
    "    return preds, loss_fn(preds, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer state and function.\n",
    "We pass the `trainables` to `sgd_init` to initialize an optimizer state.\n",
    "\n",
    "We create a function that takes in `trainable` gradients and an `optim_state`,\n",
    "and returns updates to be applied to the `trainables` and a new `optim_state`.\n",
    "\n",
    "Note we used `jax.tree_util.Partial` instead of `functools.Partial` when\n",
    "defining the `optim_fn` to allow to be passed to jit-compiled functions, notably\n",
    "`train_step`.\n",
    "Read more about this [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.Partial.html?highlight=Partial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_state = sgd_init(trainables)\n",
    "optim_fn = tree_util.Partial(sgd_step, lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training step.\n",
    "We  use JAX's `value_and_grad` to calculate the batch loss and\n",
    "gradients with respect to the `trainables`. Read more about JAX's autodiff\n",
    "[here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#taking-derivatives-with-grad).\n",
    "\n",
    "The batch loss is only used for logging, but the gradients are passed to\n",
    "`optim_fn` to get update gradients and a new `optim_state`.\n",
    "\n",
    "We apply the update gradient on the model weights.\n",
    "\n",
    "Finally, we return the batch loss, new `trainables`, and the new `optim_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tree_util.Partial(jax.jit, static_argnames=\"hyperparams\")\n",
    "def train_step(\n",
    "    x_batch: np.array, \n",
    "    y_batch: np.array,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state\n",
    "):\n",
    "    # Find batch loss and gradients\n",
    "    (loss, non_trainables), gradients = jax.value_and_grad(\n",
    "        model_trainig_loss,\n",
    "        argnums=2, # gradients wrt trainables (argument 2 of model_loss)\n",
    "        has_aux=True # non_trainables is auxiliary data, loss is the true ouput\n",
    "    )(x_batch, y_batch, trainables, non_trainables, hyperparams)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    gradients, optim_state = optim_fn(gradients, optim_state)\n",
    "\n",
    "    # Update model_weights with new gradients\n",
    "    trainables = apply_updates(gradients, trainables)\n",
    "    return loss, trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for training and testing loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    X_train, y_train,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state\n",
    "):\n",
    "    num_batches = len(X_train)\n",
    "    train_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_train[i], y_train[i]\n",
    "        loss, trainables, non_trainables, optim_state = train_step(\n",
    "            x_batch, y_batch,\n",
    "            trainables, non_trainables, hyperparams,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / num_batches}\") \n",
    "    return trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams\n",
    "):\n",
    "    num_batches = len(X_test)\n",
    "    test_loss, accuracy = 0, 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_test[i], y_test[i]\n",
    "        preds, loss = model_inference_preds_loss(\n",
    "            x_batch, y_batch, trainables, non_trainables, hyperparams\n",
    "        )\n",
    "        test_loss += loss\n",
    "        accuracy += (jnp.argmax(preds, axis=1) == y_batch).sum() / len(x_batch)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / num_batches}, accuracy: {accuracy / num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        trainables, non_trainables, optim_state = train_epoch(\n",
    "            X_train, y_train,\n",
    "            trainables, non_trainables, hyperparams,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(X_test, y_test, trainables, non_trainables, hyperparams)\n",
    "        print(f\"----------------\")\n",
    "    \n",
    "    return trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MLP on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 0.45100072026252747\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.2034211903810501\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.14783120155334473\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.11520185321569443\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.09326479583978653\n",
      "Test loss: 0.09866884350776672, accuracy: 0.9701344966888428\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.0771799311041832\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.06497106701135635\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.055386949330568314\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.04769798368215561\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.041314348578453064\n",
      "Test loss: 0.07340061664581299, accuracy: 0.9761669635772705\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.03593740612268448\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.03138212487101555\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.02742491476237774\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.02399240806698799\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.02098463848233223\n",
      "Test loss: 0.06658335030078888, accuracy: 0.9792326092720032\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.018391085788607597\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.016208093613386154\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.014269006438553333\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.012606671079993248\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.011114835739135742\n",
      "Test loss: 0.06465861201286316, accuracy: 0.9801226258277893\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.009824499487876892\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.008682957850396633\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.007692941930145025\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.006850344594568014\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.006128341890871525\n",
      "Test loss: 0.06422598659992218, accuracy: 0.9813093543052673\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.005497577600181103\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.004956514108926058\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.004499723669141531\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.004100861493498087\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.0037554132286459208\n",
      "Test loss: 0.06412950158119202, accuracy: 0.9822982549667358\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_trainables, new_non_trainables, new_optim_state = train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state,\n",
    "    30, 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c3d7272c1eba356ec9149ec42daf5acdf55d6fdb447aefce6509807a5e73802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
