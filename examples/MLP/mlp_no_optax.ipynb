{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation in mlax without Optax optimizers.\n",
    "This notebook just uses the `mlax` package.\n",
    "\n",
    "You can view the Pytorch reference implementation in `mlp_reference.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import (\n",
    "    numpy as jnp,\n",
    "    nn,\n",
    "    random\n",
    ")\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax.nn import Series, Linear, Bias, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local python file containing an SGD optimizer written in JAX.\n",
    "from optim import (\n",
    "    sparse_categorical_crossentropy,\n",
    "    sgd_init,\n",
    "    sgd_step,\n",
    "    apply_updates\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and batch the MNIST datasets.\n",
    "We follow this example\n",
    "[Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html) in using Pytorch dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class ToNumpy:\n",
    "  def __call__(self, pic):\n",
    "    return np.array(pic)\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root=\"../data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToNumpy()\n",
    ")\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    root=\"../data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToNumpy()\n",
    ")\n",
    "print(mnist_train.data.shape)\n",
    "print(mnist_test.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469 79\n"
     ]
    }
   ],
   "source": [
    "def numpy_collate(batch):\n",
    "  if isinstance(batch[0], np.ndarray):\n",
    "    return np.stack(batch)\n",
    "  elif isinstance(batch[0], (tuple,list)):\n",
    "    transposed = zip(*batch)\n",
    "    return [numpy_collate(samples) for samples in transposed]\n",
    "  else:\n",
    "    return np.array(batch)\n",
    "\n",
    "batch_size=128\n",
    "train_dataloader = DataLoader(\n",
    "    mnist_train, batch_size=128, shuffle=True, collate_fn=numpy_collate, num_workers=6\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    mnist_test, batch_size=128, collate_fn=numpy_collate, num_workers=6\n",
    ")\n",
    "print(len(train_dataloader), len(test_dataloader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MLP using `mlax.module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "keys_iter = iter([random.fold_in(random.PRNGKey(0), i) for i in range(6)])\n",
    "model = Series([\n",
    "    F(lambda x: jnp.reshape(x.astype(jnp.float32) / 255.0, (-1,))),  # Flatten and scale\n",
    "    Linear(next(keys_iter), out_features=512),\n",
    "    Bias(next(keys_iter), in_features=512),\n",
    "    F(nn.relu),\n",
    "    Linear(next(keys_iter), out_features=512),\n",
    "    Bias(next(keys_iter), in_features=512),\n",
    "    F(nn.relu),\n",
    "    Linear(next(keys_iter), out_features=10),\n",
    "    Bias(next(keys_iter), in_features=10),\n",
    "    F(nn.softmax)\n",
    "])\n",
    "\n",
    "# Induce lazy initialization\n",
    "for X, _ in train_dataloader:\n",
    "    activations, _ = model(X[0], None, inference_mode=True)\n",
    "    print(activations.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = sparse_categorical_crossentropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_state = sgd_init(model.filter())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(X, y, model, optim_state):\n",
    "    def _model_loss(X, y, trainables, non_trainables):\n",
    "        model = trainables.combine(non_trainables)\n",
    "        preds, model = jax.vmap(\n",
    "            model.__call__,\n",
    "            in_axes = (0, None, None, None),\n",
    "            out_axes = (0, None),\n",
    "            axis_name = \"N\"\n",
    "        )(X, None, False, \"N\")\n",
    "        return loss_fn(preds, y), model\n",
    "\n",
    "    # Find batch loss and gradients with resect to trainables\n",
    "    trainables, non_trainables = model.partition()\n",
    "    (loss, model), gradients = jax.value_and_grad(\n",
    "        _model_loss,\n",
    "        argnums=2, # gradients wrt trainables (argument 2 of model_training_loss)\n",
    "        has_aux=True # model is auxiliary data, loss is the true ouput\n",
    "    )(X, y, trainables, non_trainables)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    gradients, optim_state = sgd_step(gradients, optim_state)\n",
    "\n",
    "    # Update parameters with new gradients\n",
    "    trainables, non_trainables = model.partition()\n",
    "    trainables = apply_updates(gradients, trainables)\n",
    "    return loss, trainables.combine(non_trainables), optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def test_step(X, y, model):\n",
    "    preds, _ = jax.vmap(\n",
    "        model.__call__,\n",
    "        in_axes = (0, None, None, None),\n",
    "        out_axes = (0, None),\n",
    "        axis_name = \"N\"\n",
    "    )(X, None, True, \"N\")\n",
    "    accurate = (jnp.argmax(preds, axis=1) == y).sum()\n",
    "    return loss_fn(preds, y), accurate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optim_state):\n",
    "    train_loss = 0.0\n",
    "    for X, y in dataloader:\n",
    "        loss, model, optim_state = train_step(X, y, model, optim_state)\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / len(dataloader)}\") \n",
    "    return model, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    test_loss, accurate = 0.0, 0\n",
    "    for X, y in dataloader:\n",
    "        loss, acc = test_step(X, y, model)\n",
    "        test_loss += loss\n",
    "        accurate += acc\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / len(dataloader)}, accuracy: {accurate / len(dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    model,\n",
    "    optim_state,\n",
    "    epochs,\n",
    "    test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        model, optim_state = train_epoch(train_dataloader, model, optim_state)\n",
    "        if (epoch % test_every == 0):\n",
    "            test(test_dataloader, model)\n",
    "        print(f\"----------------\")\n",
    "    return model, optim_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 0.43574488162994385\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.19324670732021332\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.1411539912223816\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.11070062220096588\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.08962983638048172\n",
      "Test loss: 0.09407661855220795, accuracy: 0.9695000648498535\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.0755179151892662\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.0630074217915535\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.054314710199832916\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.04635758325457573\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.04083261638879776\n",
      "Test loss: 0.06593061238527298, accuracy: 0.9785000681877136\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.034974969923496246\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.030560683459043503\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.027007773518562317\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.022974111139774323\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.02041763626039028\n",
      "Test loss: 0.058662258088588715, accuracy: 0.9816000461578369\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.01744050532579422\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.015302663668990135\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.01342378556728363\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.011797960847616196\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.010545704513788223\n",
      "Test loss: 0.05927324295043945, accuracy: 0.9808000326156616\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.009176083840429783\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.00802089273929596\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.007372966036200523\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.006524102296680212\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.005725270137190819\n",
      "Test loss: 0.060127634555101395, accuracy: 0.9816000461578369\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.005303265526890755\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.004816220607608557\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.004390764515846968\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.00404771976172924\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.0037360964342951775\n",
      "Test loss: 0.06188604235649109, accuracy: 0.9818000197410583\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "with jax.default_matmul_precision(\"float32\"):\n",
    "    new_params, new_optim_state = train_loop(\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        model,\n",
    "        optim_state,\n",
    "        30, 5\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c3d7272c1eba356ec9149ec42daf5acdf55d6fdb447aefce6509807a5e73802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
