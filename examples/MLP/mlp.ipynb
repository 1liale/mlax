{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation in mlax with Optax optimizers.\n",
    "This notebook uses the [Optax](https://optax.readthedocs.io/en/latest/optax-101.html) JAX optimization library.\n",
    "\n",
    "You can view the Pytorch reference implementation in `mlp_reference.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import (\n",
    "    nn,\n",
    "    random,\n",
    "    tree_util\n",
    ")\n",
    "import numpy as np\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import `Linear`, `Bias`, and `F` transformations from `mlax.nn` to build some\n",
    "dense layers.\n",
    "\n",
    "We import `Series` from `mlax.block` to stack the dense layers into an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax.nn import Linear, Bias, F\n",
    "from mlax.block import Series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import helpers to load data from Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import batch, load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and batch the MNIST datasets.\n",
    "We use helper functions to load in Pytorch datasets as numpy and convert them in\n",
    "to lists containing the batches.\n",
    "\n",
    "Checkout\n",
    "[Training a Simple Neural Network with tensorflow/datasets Data Loading](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html) and\n",
    "[Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)\n",
    "for other ways to load in Tensorflow and Pytorch datsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n",
      "469 469\n",
      "79 79\n"
     ]
    }
   ],
   "source": [
    "# Load in datasets with helper\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(\"../data\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Batch datasets\n",
    "batch_size = 128\n",
    "X_train, y_train = batch(X_train, y_train, batch_size)\n",
    "X_test, y_test = batch(X_test, y_test, batch_size)\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLP model parameters.\n",
    "`model_init` consumes a `jax.random.PRNGKey` when initializing the parameters.\n",
    "Read more about random numbers in JAX [here](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html).\n",
    "\n",
    "`F` is a wrapper around a stateless function. `Linear` is a linear\n",
    "transformation without bias. `Bias` adds a bias term.\n",
    "\n",
    "Each `init` function returns a `trainables`, `non_trainables`, and\n",
    "`hyperparams`.\n",
    "\n",
    "`trainables` are trainable weights. `non_trainables` are non-trainable\n",
    "variables. `hyperparams` are additional parameters required by the forward pass.\n",
    "\n",
    "The `trainables` and `non_trainables` are PyTrees of JAX arrays. Read more about\n",
    "JAX PyTrees [here](https://jax.readthedocs.io/en/latest/pytrees.html).\n",
    "\n",
    "`hyperparams` is a NamedTuple containing Python types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(key):\n",
    "    keys_iter = iter(random.split(key, 6))\n",
    "    return Series.init(\n",
    "        # Convert int8 numpy inputs to float32 JAX arrays and flatten them\n",
    "        F.init(lambda x:jnp.reshape(\n",
    "            jnp.asarray(x, jnp.float32) / 255.0,\n",
    "            (len(x), -1))\n",
    "        ),\n",
    "\n",
    "        # Dense layer with relu activation\n",
    "        Linear.init(\n",
    "            key=next(keys_iter),\n",
    "            in_features=28 * 28,\n",
    "            out_features=512,\n",
    "        ),\n",
    "        Bias.init(\n",
    "            key=next(keys_iter),\n",
    "            in_feature_shape=(512,)\n",
    "        ),\n",
    "        F.init(nn.relu),\n",
    "\n",
    "        # Dense layer with relu activation\n",
    "        Linear.init(next(keys_iter), 512, 512),\n",
    "        Bias.init(next(keys_iter), (512,)),\n",
    "        F.init(nn.relu),\n",
    "        \n",
    "        # Dense layer without activation or softmax\n",
    "        Linear.init(next(keys_iter), 512, 10),\n",
    "        Bias.init(next(keys_iter), (10,)),\n",
    "    )\n",
    "\n",
    "trainables, non_trainables, hyperparams = model_init(random.PRNGKey(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MLP dataflow.\n",
    "`Series.fwd` takes in batched input features and tuples of `trainables`,\n",
    "`non_trainables`, and `hyperparams`. It figures out which layer each\n",
    "`hyperparams` is for, and calls their forward pass functions on the input\n",
    "features in sequence.\n",
    "\n",
    "It returns the model predictions and updated `non_trainables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fwd = Series.fwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    batched_preds: jnp.array,\n",
    "    batched_targets: np.array\n",
    "):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(\n",
    "        batched_preds,\n",
    "        batched_targets\n",
    "    ).mean() # Optax returns per-example loss, this returns the mean batch loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two convenience functions.\n",
    "\n",
    "``model_training_loss`` returns the batch loss and updated `non_trainables` from\n",
    "batched inputs and targets.\n",
    "\n",
    "``model_inference_preds_loss`` returns the predictions and batch loss from\n",
    "batched inputs and targets.\n",
    "\n",
    "We jit-compile the ``model_inference_preds_loss`` for significant speedups. Note\n",
    "that `hyperparams` is a static argument because it is made of Python types, not\n",
    "valid JAX types, and it also used interally for control flow. Read more about\n",
    "jit-compilation [here](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainig_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    trainables,\n",
    "    non_trainables,\n",
    "    hyperparams,\n",
    "):\n",
    "    preds, non_trainables = model_fwd(\n",
    "        x_batch, trainables, non_trainables, hyperparams\n",
    "    )\n",
    "    return loss_fn(preds, y_batch), non_trainables\n",
    "\n",
    "@tree_util.Partial(jax.jit, static_argnames=\"hyperparams\")\n",
    "def model_inference_preds_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    trainables,\n",
    "    non_trainables,\n",
    "    hyperparams\n",
    "):\n",
    "    preds, _ = model_fwd(\n",
    "        x_batch, trainables, non_trainables, hyperparams\n",
    "    )\n",
    "    return preds, loss_fn(preds, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer using Optax.\n",
    "We pass the `trainables` to `init` to initialize an optimizer state.\n",
    "\n",
    "We define a function that takes in `trainables` gradients and an `optim_state`,\n",
    "and returns updates to be applied on `trainables` and a new `optim_state`.\n",
    "\n",
    "Note we used `jax.tree_util.Partial` to wrap the `optim_fn`. Doing so allows the\n",
    "`optim_fn` to be passed to jit-compiled functions, notably `train_step`.\n",
    "Read more about this [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.Partial.html?highlight=Partial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.sgd(1e-2, momentum=0.9)\n",
    "optim_state = optimizer.init(trainables)\n",
    "optim_fn = tree_util.Partial(optimizer.update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training step.\n",
    "We  use JAX's `value_and_grad` to calculate the batch loss and\n",
    "gradients with respect to the `trainables`. Read more about JAX's autodiff\n",
    "[here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#taking-derivatives-with-grad).\n",
    "\n",
    "The batch loss is only used for logging, but the gradients are passed to\n",
    "`optim_fn` to get update gradients and a new `optim_state`.\n",
    "\n",
    "We apply the update gradient on the model weights.\n",
    "\n",
    "Finally, we return the batch loss, new `trainables`, and the new `optim_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tree_util.Partial(jax.jit, static_argnames=\"hyperparams\")\n",
    "def train_step(\n",
    "    x_batch: np.array, \n",
    "    y_batch: np.array,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state\n",
    "):\n",
    "    # Find batch loss and gradients\n",
    "    (loss, non_trainables), gradients = jax.value_and_grad(\n",
    "        model_trainig_loss,\n",
    "        argnums=2, # gradients wrt trainables (argument 2 of model_loss)\n",
    "        has_aux=True # non_trainables is auxiliary data, loss is the true ouput\n",
    "    )(x_batch, y_batch, trainables, non_trainables, hyperparams)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    gradients, optim_state = optim_fn(gradients, optim_state)\n",
    "\n",
    "    # Update model_weights with new gradients\n",
    "    trainables = optax.apply_updates(gradients, trainables)\n",
    "    return loss, trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for training and testing loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    X_train, y_train,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state\n",
    "):\n",
    "    num_batches = len(X_train)\n",
    "    train_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_train[i], y_train[i]\n",
    "        loss, trainables, non_trainables, optim_state = train_step(\n",
    "            x_batch, y_batch,\n",
    "            trainables, non_trainables, hyperparams,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / num_batches}\") \n",
    "    return trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams\n",
    "):\n",
    "    num_batches = len(X_test)\n",
    "    test_loss, accuracy = 0, 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_test[i], y_test[i]\n",
    "        preds, loss = model_inference_preds_loss(\n",
    "            x_batch, y_batch, trainables, non_trainables, hyperparams\n",
    "        )\n",
    "        test_loss += loss\n",
    "        accuracy += (jnp.argmax(preds, axis=1) == y_batch).sum() / len(x_batch)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / num_batches}, accuracy: {accuracy / num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        trainables, non_trainables, optim_state = train_epoch(\n",
    "            X_train, y_train,\n",
    "            trainables, non_trainables, hyperparams,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(X_test, y_test, trainables, non_trainables, hyperparams)\n",
    "        print(f\"----------------\")\n",
    "    \n",
    "    return trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MLP on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 0.4509953260421753\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.20344839990139008\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.14795967936515808\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.11533735692501068\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.09333495795726776\n",
      "Test loss: 0.09862024337053299, accuracy: 0.969936728477478\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.07738758623600006\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.06509795039892197\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.05554154887795448\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.04781549423933029\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.041424255818128586\n",
      "Test loss: 0.07369294762611389, accuracy: 0.9758702516555786\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.036050014197826385\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.03146369010210037\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.02747494727373123\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.024036575108766556\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.021037571132183075\n",
      "Test loss: 0.06629890948534012, accuracy: 0.9791337251663208\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.01841382309794426\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.01618640497326851\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.01423928327858448\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.012547163292765617\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.011060697957873344\n",
      "Test loss: 0.06447350978851318, accuracy: 0.9801226258277893\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.009775451384484768\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.00863032415509224\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.007667344994843006\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.006838138680905104\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.006112860050052404\n",
      "Test loss: 0.06404562294483185, accuracy: 0.981803834438324\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.005486536305397749\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.004957554396241903\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.004491094965487719\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.004092327319085598\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.0037531873676925898\n",
      "Test loss: 0.06387492269277573, accuracy: 0.9825949668884277\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_trainables, new_non_trainables, new_optim_state = train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state,\n",
    "    30, 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c3d7272c1eba356ec9149ec42daf5acdf55d6fdb447aefce6509807a5e73802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
