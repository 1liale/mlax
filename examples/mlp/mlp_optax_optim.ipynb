{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation in mlax with Optax optimizers.\n",
    "To help with understaning, consider first reading the tutorial on\n",
    "[Stateful Computations in JAX](https://jax.readthedocs.io/en/latest/jax-101/07-state.html),\n",
    "especially the Linear Regression worked example. \n",
    "\n",
    "Also consider going over Optax's [Quick Start](https://optax.readthedocs.io/en/latest/optax-101.html).\n",
    "\n",
    "You can view the Pytorch reference implementation in `mlp_reference.ipynb`.\n",
    "\n",
    "See `mlp_no_optax.ipynb` for an implementation without using Optax.\n",
    "\n",
    "See `mlp_optax_loss.ipynb` for an example using only Optax loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import (\n",
    "    nn,\n",
    "    random,\n",
    "    tree_util\n",
    ")\n",
    "import numpy as np\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the `Linear` block from `mlax.nn.blocks` to build the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax.blocks import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import helpers to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import batch, load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST datasets as numpy arrays.\n",
    "We could instead load the datasets as JAX arrays. If we did so, the dataset\n",
    "will be sent uncommitted to the default device, in my case, the GPU.\n",
    "Read more about this [here](https://jax.readthedocs.io/en/latest/faq.html#controlling-data-and-computation-placement-on-devices).\n",
    "\n",
    "Since most datasets are too expensive to copy to to each accelerator, they\n",
    "should stay on the CPU and be streamed to the accelerators during training and\n",
    "testing. While we could do this by using `jax.device_put()` on JAX arrays, it's\n",
    "simpler to keep them as numpy arrays, which always stay on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_mnist(\"../data\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch the datasets\n",
    "Data stay as numpy arrays, and therefore on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 938\n",
      "157 157\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "X_train, y_train = batch(X_train, y_train, batch_size)\n",
    "X_test, y_test = batch(X_test, y_test, batch_size)\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are more elegant ways to load in Tensorflow or Pytorch data.\n",
    "\n",
    "Consider reading Jax's\n",
    "[Training a Simple Neural Network with tensorflow/datasets Data Loading](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html) and\n",
    "[Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLP model weights.\n",
    "`model_init` uses a `jax.random.PRNGKey` when initializing the weights.\n",
    "Read more about random numbers in JAX [here](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html).\n",
    "\n",
    "The initialized `model_weights` are JAX arrays, so they reside on the default\n",
    "device, in my case the GPU.\n",
    "\n",
    "Furthermore, default weights of `mlax` transformations and blocks are of the type\n",
    "`float32`. You can override that with `init` functions' `dtype` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(key):\n",
    "    key1, key2, key3 = random.split(key, 3)\n",
    "    \n",
    "    # Initialize weights for each linear block on the GPU, default type: float32\n",
    "    w1 = Linear.init(key1, 28*28, 512)\n",
    "    w2 = Linear.init(key2, 512, 512)\n",
    "    w3 = Linear.init(key3, 512, 10)\n",
    "    return [w1, w2, w3]\n",
    "\n",
    "model_weights = model_init(random.PRNGKey(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MLP dataflow\n",
    "We first flatten the 2D numpy array with `jax.numpy.reshape`, converting it\n",
    "into a JAX array , which gets sent to the default device, in my case the GPU.\n",
    "\n",
    "We also explicitly convert its type to `float32`, the type of our\n",
    "model weights.\n",
    "\n",
    "Knowing what types we are working with is important because mlax functions do\n",
    "not promote types implicitly.\n",
    "\n",
    "We then pass the flattened inputs through two `mlax.nn.Linear` blocks, but\n",
    "the last one does end with an activation.\n",
    "\n",
    "mlax functions, including `mlax.nn.Linear`, only accept a single unbatched\n",
    "sample as input. So `model_fwd` only works on single unbatched samples. So we\n",
    "use  JAX's [vmap](https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html)\n",
    "to define a `batched_model_fwd`, which works on batched inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def model_fwd(\n",
    "    x: np.array,\n",
    "    weights: jnp.array\n",
    "):\n",
    "    # Flatten numpy array and send it to GPU.\n",
    "    x = jnp.reshape(x, (-1, ))\n",
    "    # Explicit type promotion for the following mlax functions\n",
    "    x = x.astype(\"float32\") \n",
    "\n",
    "    w1, w2, w3 = weights\n",
    "    x = Linear.fwd(x, w1, nn.relu)\n",
    "    x = Linear.fwd(x, w2, nn.relu)\n",
    "    x = Linear.fwd(x, w3, None)\n",
    "    return x\n",
    "\n",
    "batched_model_fwd = jax.vmap(model_fwd, in_axes=[0, None])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define bach loss function\n",
    "We use Optax's crossentropy loss, which is already vectorized (batched).\n",
    "\n",
    "Note that target values are numpy arrays, which gets streamed from CPU and gets\n",
    "converted into a JAX array on the GPU implicitly by Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def batched_loss(\n",
    "    batched_preds: jnp.array,\n",
    "    batched_targets: np.array\n",
    "):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(\n",
    "        batched_preds,\n",
    "        batched_targets\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a convenience function that calculates the model loss based on\n",
    "datasets' batched inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_model_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    weights: jnp.array\n",
    "):\n",
    "    return batched_loss(batched_model_fwd(x_batch, weights), y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer state and function\n",
    "We use Optax to create an optimizer.\n",
    "\n",
    "We create an optimizer state and optimizer function.\n",
    "\n",
    "Note we used `jax.tree_util.Partial` to wrap the update function. Doing this\n",
    "allows the update function to be used in `jit` compiled functions.\n",
    "Read more about this [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.Partial.html?highlight=Partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer state on the GPU\n",
    "optimizer = optax.sgd(1e-3, momentum=0.7)\n",
    "optim_state = optimizer.init(model_weights)\n",
    "optim_fn = tree_util.Partial(optimizer.update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training step\n",
    "We first calculate the batch loss, which will only be used for logging.\n",
    "\n",
    "We then use JAX's `value_and_grad` to calculate the batch loss and the\n",
    "gradients with repect to `model_weights`. Read more about JAX's autodiff\n",
    "[here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#taking-derivatives-with-grad).\n",
    "\n",
    "The batch loss is only used for logging, but the gradients are passed to\n",
    "`optim_fn` to get new gradients and a new optimizer state.\n",
    "\n",
    "We apply the new gradients to update the model weights.\n",
    "\n",
    "Finally, we return the batch loss, new `model_weights`, and new `optim_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(\n",
    "    x_batch: np.array, \n",
    "    y_batch: np.array,\n",
    "    model_weights: jnp.array,\n",
    "    optim_fn, # (gradients, optim_state) -> (new_gradients, new_optim_state)\n",
    "    optim_state: jnp.array\n",
    "):\n",
    "    # Find batch loss and gradients\n",
    "    loss, gradients = jax.value_and_grad(\n",
    "        batched_model_loss,\n",
    "        argnums=2 # gradients wrt model_weights (argument 2)\n",
    "    )(x_batch, y_batch, model_weights)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    gradients, optim_state = optim_fn(gradients, optim_state)\n",
    "\n",
    "    # Update model_weights with new gradients\n",
    "    model_weights = optax.apply_updates(gradients, model_weights)\n",
    "    return loss, model_weights, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X_train, y_train, model_weights, optim_fn, optim_state):\n",
    "    num_batches = len(X_train)\n",
    "    train_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_train[i], y_train[i]\n",
    "        loss, model_weights, optim_state = train_step(\n",
    "            x_batch, y_batch,\n",
    "            model_weights,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / num_batches}\") \n",
    "    return model_weights, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, y_test, model_weights):\n",
    "    num_batches = len(X_test)\n",
    "    test_loss, accuracy = 0, 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_test[i], y_test[i]\n",
    "        preds = batched_model_fwd(\n",
    "            x_batch, model_weights\n",
    "        )\n",
    "        loss = batched_loss(preds, y_batch)\n",
    "        test_loss += loss\n",
    "        accuracy += (jnp.argmax(preds, axis=1) == y_batch).sum() / len(x_batch)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / num_batches}, accuracy: {accuracy / num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model_weights,\n",
    "    optim_fn, optim_state,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        model_weights, optim_state = train_epoch(\n",
    "            X_train, y_train,\n",
    "            model_weights,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(X_test, y_test, model_weights)\n",
    "        print(f\"----------------\")\n",
    "    \n",
    "    return model_weights, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MLP on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 2.0185959339141846\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.1646140217781067\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.10156477242708206\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.06812717765569687\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.04950431361794472\n",
      "Test loss: 0.1562071591615677, accuracy: 0.9608877301216125\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.03605528175830841\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.02721119113266468\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.02074531279504299\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.015856629237532616\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.012181155383586884\n",
      "Test loss: 0.14902129769325256, accuracy: 0.9658638834953308\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.009694949723780155\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.007761626038700342\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.006351014133542776\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.00523013062775135\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.004337526857852936\n",
      "Test loss: 0.1534748524427414, accuracy: 0.9669586420059204\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.003690859070047736\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.0031652869656682014\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.0027446579188108444\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.0024040555581450462\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.0021318832878023386\n",
      "Test loss: 0.15962637960910797, accuracy: 0.9676552414894104\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.0019087021937593818\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.0017251897370442748\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.0015611550770699978\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.001428423449397087\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.0013069470878690481\n",
      "Test loss: 0.16352058947086334, accuracy: 0.968252420425415\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.0012136446312069893\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.0011247503571212292\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.0010499402415007353\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.0009830310009419918\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.0009228595881722867\n",
      "Test loss: 0.16641385853290558, accuracy: 0.9689490795135498\n",
      "----------------\n",
      "Epoch 31\n",
      "----------------\n",
      "Train loss: 0.0008711983100511134\n",
      "----------------\n",
      "Epoch 32\n",
      "----------------\n",
      "Train loss: 0.0008228117949329317\n",
      "----------------\n",
      "Epoch 33\n",
      "----------------\n",
      "Train loss: 0.0007801495958119631\n",
      "----------------\n",
      "Epoch 34\n",
      "----------------\n",
      "Train loss: 0.0007409565150737762\n",
      "----------------\n",
      "Epoch 35\n",
      "----------------\n",
      "Train loss: 0.0007068210397846997\n",
      "Test loss: 0.16877198219299316, accuracy: 0.9696457386016846\n",
      "----------------\n",
      "Epoch 36\n",
      "----------------\n",
      "Train loss: 0.0006730453460477293\n",
      "----------------\n",
      "Epoch 37\n",
      "----------------\n",
      "Train loss: 0.0006445772014558315\n",
      "----------------\n",
      "Epoch 38\n",
      "----------------\n",
      "Train loss: 0.0006158353062346578\n",
      "----------------\n",
      "Epoch 39\n",
      "----------------\n",
      "Train loss: 0.0005915828514844179\n",
      "----------------\n",
      "Epoch 40\n",
      "----------------\n",
      "Train loss: 0.0005678011220879853\n",
      "Test loss: 0.17090781033039093, accuracy: 0.9704418778419495\n",
      "----------------\n",
      "Epoch 41\n",
      "----------------\n",
      "Train loss: 0.0005465151625685394\n",
      "----------------\n",
      "Epoch 42\n",
      "----------------\n",
      "Train loss: 0.0005256020813249052\n",
      "----------------\n",
      "Epoch 43\n",
      "----------------\n",
      "Train loss: 0.000507054734043777\n",
      "----------------\n",
      "Epoch 44\n",
      "----------------\n",
      "Train loss: 0.0004897262551821768\n",
      "----------------\n",
      "Epoch 45\n",
      "----------------\n",
      "Train loss: 0.00047292126691900194\n",
      "Test loss: 0.17260581254959106, accuracy: 0.9705414175987244\n",
      "----------------\n",
      "Epoch 46\n",
      "----------------\n",
      "Train loss: 0.0004570868331938982\n",
      "----------------\n",
      "Epoch 47\n",
      "----------------\n",
      "Train loss: 0.00044303268077783287\n",
      "----------------\n",
      "Epoch 48\n",
      "----------------\n",
      "Train loss: 0.00042910288902930915\n",
      "----------------\n",
      "Epoch 49\n",
      "----------------\n",
      "Train loss: 0.00041594012873247266\n",
      "----------------\n",
      "Epoch 50\n",
      "----------------\n",
      "Train loss: 0.00040433768299408257\n",
      "Test loss: 0.17412255704402924, accuracy: 0.9706409573554993\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_model_weights, new_optim_state = train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model_weights,\n",
    "    optim_fn, optim_state,\n",
    "    50, 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c2eae4f387f48c6f060ba94db73f9ed1c3f6b20f388481d7647fcca09c0c17a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
