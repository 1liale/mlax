{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation in mlax with Optax loss.\n",
    "To help with understaning, consider first reading the tutorial on\n",
    "[Stateful Computations in JAX](https://jax.readthedocs.io/en/latest/jax-101/07-state.html),\n",
    "especially the Linear Regression worked example. \n",
    "\n",
    "You can view the Pytorch reference implementation in `mlp_reference.ipynb`.\n",
    "\n",
    "This implementation uses Optax loss functions but mlax optimizers.\n",
    "\n",
    "See `mlp_no_optax.ipynb` for an implementation without using Optax.\n",
    "\n",
    "See `mlp_optax_optim.ipynb` for an example also using Optax optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import (\n",
    "    nn,\n",
    "    random,\n",
    "    tree_util\n",
    ")\n",
    "import numpy as np\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the `Linear` block from `mlax.nn.blocks` to build the MLP.\n",
    "\n",
    "We import the `sgd` optimizer to from `mlax.optim` calculate updates on the\n",
    "model weights, and we import the `apply_gradient` function to apply those\n",
    "updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax.blocks import Linear\n",
    "from mlax.optim import sgd, apply_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import helpers to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import batch, load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset as numpy arrays.\n",
    "We could instead load the datasets as JAX arrays. If we did so, the dataset\n",
    "will be sent uncommitted to the default device, in my case, the GPU.\n",
    "Read more about this [here](https://jax.readthedocs.io/en/latest/faq.html#controlling-data-and-computation-placement-on-devices).\n",
    "\n",
    "Since most datasets are too expensive to copy to to each accelerator, they\n",
    "should stay on the CPU and be streamed to the accelerators during training and\n",
    "testing. While we could do this by using `jax.device_put()` on JAX arrays, it's\n",
    "simpler to keep them as numpy arrays, which always stay on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_mnist(\"../data\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch the datasets\n",
    "Data stay as numpy arrays, and therefore on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 938\n",
      "157 157\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "X_train, y_train = batch(X_train, y_train, batch_size)\n",
    "X_test, y_test = batch(X_test, y_test, batch_size)\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are more elegant ways to load in Tensorflow or Pytorch data.\n",
    "\n",
    "Consider reading Jax's\n",
    "[Training a Simple Neural Network with tensorflow/datasets Data Loading](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html) and\n",
    "[Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLP model weights.\n",
    "`model_init` uses a `jax.random.PRNGKey` when initializing the weights.\n",
    "Read more about random numbers in JAX [here](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html).\n",
    "\n",
    "The initialized `model_weights` are JAX arrays, so they reside on the default\n",
    "device, in my case the GPU.\n",
    "\n",
    "Furthermore, default weights of `mlax` transformations and blocks are of the type\n",
    "`float32`. You can override that with `init` functions' `dtype` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(key):\n",
    "    key1, key2, key3 = random.split(key, 3)\n",
    "    \n",
    "    # Initialize weights for each linear block on the GPU, default type: float32\n",
    "    w1 = Linear.init(key1, 28*28, 512)\n",
    "    w2 = Linear.init(key2, 512, 512)\n",
    "    w3 = Linear.init(key3, 512, 10)\n",
    "    return [w1, w2, w3]\n",
    "\n",
    "model_weights = model_init(random.PRNGKey(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MLP dataflow\n",
    "\n",
    "We first explicitly convert its type to `float32`, the type of our\n",
    "model weights.\n",
    "\n",
    "We then flatten the 2D numpy array with `jax.numpy.reshape`, converting it\n",
    "into a JAX array, which gets sent to the default device, in my case the GPU.\n",
    "\n",
    "Knowing what types we are working with is important because mlax functions do\n",
    "not promote types implicitly.\n",
    "\n",
    "We then pass the flattened inputs through several `mlax.nn.Linear` blocks.\n",
    "\n",
    "Note that the last block does not end with a softmax function, as the Optax loss\n",
    "function we will use later takes in logits, not probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def model_fwd(\n",
    "    x_batch: np.array,\n",
    "    weights: jnp.array\n",
    "):\n",
    "\n",
    "    # Explicit type promotion\n",
    "    x_batch = x_batch.astype(\"float32\") \n",
    "    \n",
    "    # Flatten numpy array and send it to GPU.\n",
    "    n_batches = x_batch.shape[0]\n",
    "    x_batch = jnp.reshape(x_batch, (n_batches, -1))\n",
    "\n",
    "    w1, w2, w3 = weights\n",
    "    x_batch = Linear.fwd(x_batch, w1, nn.relu)\n",
    "    x_batch = Linear.fwd(x_batch, w2, nn.relu)\n",
    "    x_batch = Linear.fwd(x_batch, w3, None)\n",
    "    return x_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define bach loss function\n",
    "We use Optax's crossentropy loss.\n",
    "\n",
    "Note that target values are numpy arrays, which gets streamed from CPU and gets\n",
    "converted into a JAX array on the GPU implicitly by Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(\n",
    "    batched_preds: jnp.array,\n",
    "    batched_targets: np.array\n",
    "):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(\n",
    "        batched_preds,\n",
    "        batched_targets\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a convenience function that calculates the model loss based on\n",
    "datasets' batched inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    weights: jnp.array\n",
    "):\n",
    "    return loss_fn(model_fwd(x_batch, weights), y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer state and function\n",
    "As with model weights, the optimzer state is a JAX array that gets sent to the\n",
    "default device.\n",
    "\n",
    "During initialization, the optimizer infers the type of its state from the\n",
    "model weights. In our case then, the optimizer state is also of dtype `float32`.\n",
    "\n",
    "We create an optimizer function that takes in gradients and an optimizer state,\n",
    "and returns new gradients to be applied and a new optimizer state.\n",
    "\n",
    "Note we used `jax.tree_util.Partial` instead of `functools.Partial` to allow\n",
    "using this function in `jit` compiled functions. Read more about this [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.Partial.html?highlight=Partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer state on the GPU\n",
    "optim_state = sgd.init(model_weights)\n",
    "optim_fn = tree_util.Partial(sgd.step, lr=5e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training step\n",
    "We first calculate the batch loss, which will only be used for logging.\n",
    "\n",
    "We then use JAX's `value_and_grad` to calculate the batch loss and the\n",
    "gradients with repect to `model_weights`. Read more about JAX's autodiff\n",
    "[here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#taking-derivatives-with-grad).\n",
    "\n",
    "The batch loss is only used for logging, but the gradients are passed to\n",
    "`optim_fn` to get new gradients and a new optimizer state.\n",
    "\n",
    "We apply the new gradients to update the model weights.\n",
    "\n",
    "Finally, we return the batch loss, new `model_weights`, and new `optim_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(\n",
    "    x_batch: np.array, \n",
    "    y_batch: np.array,\n",
    "    model_weights: jnp.array,\n",
    "    optim_fn, # (gradients, optim_state) -> (new_gradients, new_optim_state)\n",
    "    optim_state: jnp.array\n",
    "):\n",
    "    # Find batch loss and gradients\n",
    "    loss, gradients = jax.value_and_grad(\n",
    "        model_loss,\n",
    "        argnums=2 # gradients wrt model_weights (argument 2)\n",
    "    )(x_batch, y_batch, model_weights)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    gradients, optim_state = optim_fn(gradients, optim_state)\n",
    "\n",
    "    # Update model_weights with new gradients\n",
    "    model_weights = apply_updates(gradients, model_weights)\n",
    "    return loss, model_weights, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X_train, y_train, model_weights, optim_fn, optim_state):\n",
    "    num_batches = len(X_train)\n",
    "    train_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_train[i], y_train[i]\n",
    "        loss, model_weights, optim_state = train_step(\n",
    "            x_batch, y_batch,\n",
    "            model_weights,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / num_batches}\") \n",
    "    return model_weights, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, y_test, model_weights):\n",
    "    num_batches = len(X_test)\n",
    "    test_loss, accuracy = 0, 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_test[i], y_test[i]\n",
    "        preds = model_fwd(\n",
    "            x_batch, model_weights\n",
    "        )\n",
    "        loss = loss_fn(preds, y_batch)\n",
    "        test_loss += loss\n",
    "        accuracy += (jnp.argmax(preds, axis=1) == y_batch).sum() / len(x_batch)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / num_batches}, accuracy: {accuracy / num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model_weights,\n",
    "    optim_fn, optim_state,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        model_weights, optim_state = train_epoch(\n",
    "            X_train, y_train,\n",
    "            model_weights,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(X_test, y_test, model_weights)\n",
    "        print(f\"----------------\")\n",
    "    \n",
    "    return model_weights, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MLP on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 1.768882393836975\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.15430843830108643\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.09648335725069046\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.06552977859973907\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.04472680017352104\n",
      "Test loss: 0.15082146227359772, accuracy: 0.9633758068084717\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.031187959015369415\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.02150675654411316\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.015428357757627964\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.010454383678734303\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.007274230010807514\n",
      "Test loss: 0.1621200293302536, accuracy: 0.9669586420059204\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.005341990850865841\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.004018120933324099\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.0031499029137194157\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.002497876761481166\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.0020749717950820923\n",
      "Test loss: 0.16822946071624756, accuracy: 0.9696457386016846\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.0017506182193756104\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.0014960491098463535\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.0013150270096957684\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.001166350208222866\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.0010503169614821672\n",
      "Test loss: 0.17598161101341248, accuracy: 0.9702428579330444\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.0009403941803611815\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.0008464826969429851\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.000768765399698168\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.0007049919222481549\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.0006478069699369371\n",
      "Test loss: 0.1817263513803482, accuracy: 0.9706409573554993\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.000598255661316216\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.000559838255867362\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.0005242146435193717\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.0004916921025142074\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.00046600776840932667\n",
      "Test loss: 0.18548594415187836, accuracy: 0.9709395170211792\n",
      "----------------\n",
      "Epoch 31\n",
      "----------------\n",
      "Train loss: 0.0004396690637804568\n",
      "----------------\n",
      "Epoch 32\n",
      "----------------\n",
      "Train loss: 0.000417874165577814\n",
      "----------------\n",
      "Epoch 33\n",
      "----------------\n",
      "Train loss: 0.0003983558854088187\n",
      "----------------\n",
      "Epoch 34\n",
      "----------------\n",
      "Train loss: 0.0003792243078351021\n",
      "----------------\n",
      "Epoch 35\n",
      "----------------\n",
      "Train loss: 0.00036286836257204413\n",
      "Test loss: 0.188170924782753, accuracy: 0.971337616443634\n",
      "----------------\n",
      "Epoch 36\n",
      "----------------\n",
      "Train loss: 0.00034730660263448954\n",
      "----------------\n",
      "Epoch 37\n",
      "----------------\n",
      "Train loss: 0.00033374674967490137\n",
      "----------------\n",
      "Epoch 38\n",
      "----------------\n",
      "Train loss: 0.00032018544152379036\n",
      "----------------\n",
      "Epoch 39\n",
      "----------------\n",
      "Train loss: 0.0003081814502365887\n",
      "----------------\n",
      "Epoch 40\n",
      "----------------\n",
      "Train loss: 0.0002970870118588209\n",
      "Test loss: 0.1904018074274063, accuracy: 0.971337616443634\n",
      "----------------\n",
      "Epoch 41\n",
      "----------------\n",
      "Train loss: 0.0002861250250134617\n",
      "----------------\n",
      "Epoch 42\n",
      "----------------\n",
      "Train loss: 0.0002763048396445811\n",
      "----------------\n",
      "Epoch 43\n",
      "----------------\n",
      "Train loss: 0.00026716795400716364\n",
      "----------------\n",
      "Epoch 44\n",
      "----------------\n",
      "Train loss: 0.00025818412541411817\n",
      "----------------\n",
      "Epoch 45\n",
      "----------------\n",
      "Train loss: 0.000250411219894886\n",
      "Test loss: 0.1926531046628952, accuracy: 0.9714370965957642\n",
      "----------------\n",
      "Epoch 46\n",
      "----------------\n",
      "Train loss: 0.0002425311686238274\n",
      "----------------\n",
      "Epoch 47\n",
      "----------------\n",
      "Train loss: 0.00023561323178000748\n",
      "----------------\n",
      "Epoch 48\n",
      "----------------\n",
      "Train loss: 0.0002287021343363449\n",
      "----------------\n",
      "Epoch 49\n",
      "----------------\n",
      "Train loss: 0.0002223874325864017\n",
      "----------------\n",
      "Epoch 50\n",
      "----------------\n",
      "Train loss: 0.00021631720301229507\n",
      "Test loss: 0.19429229199886322, accuracy: 0.971636176109314\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_model_weights, new_optim_state = train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model_weights,\n",
    "    optim_fn, optim_state,\n",
    "    50, 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c2eae4f387f48c6f060ba94db73f9ed1c3f6b20f388481d7647fcca09c0c17a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
