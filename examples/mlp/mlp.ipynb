{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation in mlax with Optax optimizers.\n",
    "To help with understaning, consider reading the tutorial on\n",
    "[Stateful Computations in JAX](https://jax.readthedocs.io/en/latest/jax-101/07-state.html),\n",
    "especially the Linear Regression worked example. \n",
    "\n",
    "Also consider going over Optax's [Quick Start](https://optax.readthedocs.io/en/latest/optax-101.html).\n",
    "\n",
    "You can view the Pytorch reference implementation in `mlp_reference.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import (\n",
    "    nn,\n",
    "    random,\n",
    "    tree_util\n",
    ")\n",
    "import numpy as np\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import `Linear`, `Bias`, and `F` transformations from `mlax.nn` to build some\n",
    "dense layers.\n",
    "\n",
    "We import `series_fwd` from `mlax.block` to stack the dense layers into an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax.nn import Linear, Bias, F\n",
    "from mlax.block import series_fwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import helpers to load data from Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import batch, load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and batch the MNIST datasets.\n",
    "Here, we used simple helper functions to load in Pytorch datasets as numpy and\n",
    "convert them in to lists containing the batches.\n",
    "\n",
    "Checkout\n",
    "[Training a Simple Neural Network with tensorflow/datasets Data Loading](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html) and\n",
    "[Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)\n",
    "for other ways to load in Tensorflow and Pytorch datsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n",
      "938 938\n",
      "157 157\n"
     ]
    }
   ],
   "source": [
    "# Load in datasets with helper\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(\"../data\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Batch datasets\n",
    "batch_size = 64\n",
    "X_train, y_train = batch(X_train, y_train, batch_size)\n",
    "X_test, y_test = batch(X_test, y_test, batch_size)\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLP model parameters.\n",
    "`model_init` consummes a `jax.random.PRNGKey` when initializing the parameters.\n",
    "Read more about random numbers in JAX [here](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html).\n",
    "\n",
    "`F` is a wrapper around a stateless function. `Linear` is a linear\n",
    "transformation without bias. `Bias` add a bias term.\n",
    "\n",
    "Each `init` function returns a `trainables`, `non_trainables`, and\n",
    "`hyperparams`.\n",
    "\n",
    "`trainables` are trainable weights. `non_trainables` are non-trainable\n",
    "variables. `hyperparams` are additional parameters required by the forward pass.\n",
    "\n",
    "The `trainables` and `non_trainables` are PyTrees of JAX arrays. Read more about\n",
    "JAX PyTrees [here](https://jax.readthedocs.io/en/latest/pytrees.html).\n",
    "\n",
    "The initialized `hyperparams` are hashable Python types (ints, strings, tuples,\n",
    "etc., but not lists).\n",
    "\n",
    "Default `trainables` and `non_trainables` are of the type `float32`, which\n",
    "incidentally means computations will be done in `float32`. You can override that\n",
    "with `init` functions' `dtype` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(key):\n",
    "    keys_iter = iter(random.split(key, 6))\n",
    "    return zip(\n",
    "        # Convert int8 numpy inputs to float32 JAX arrays and flatten them\n",
    "        F.init(lambda x:jnp.reshape(\n",
    "            jnp.asarray(x, jnp.float32),\n",
    "            (len(x), -1))\n",
    "        ),\n",
    "\n",
    "        # Dense layer with relu activation\n",
    "        Linear.init(\n",
    "            key=next(keys_iter),\n",
    "            in_feature_shape=(28 * 28,),\n",
    "            out_feature_shape=(512,)\n",
    "        ),\n",
    "        Bias.init(\n",
    "            key=next(keys_iter),\n",
    "            in_feature_shape=(512,)\n",
    "        ),\n",
    "        F.init(nn.relu),\n",
    "\n",
    "        # Dense layer with relu activation\n",
    "        Linear.init(next(keys_iter), (512,), (512,)),\n",
    "        Bias.init(next(keys_iter), (512,)),\n",
    "        F.init(nn.relu),\n",
    "        \n",
    "        # Dense layer with no activation \n",
    "        Linear.init(next(keys_iter), (512,), (10,)),\n",
    "        Bias.init(next(keys_iter), (10,))\n",
    "    )\n",
    "\n",
    "trainables, non_trainables, hyperparams = model_init(random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MLP dataflow\n",
    "The `series_fwd` function takes in batched input features and a tuple of\n",
    "`trainables`, `non_trainables`, and `hyperparams`. It figures out which layer\n",
    "each `hyperparams` is for, and calls their forward pass functions on the input\n",
    "features in sequence.\n",
    "\n",
    "It returns the model predictions and updated `non_trainables`.\n",
    "\n",
    "We jit-compile the forward pass function for significant speedups. Note\n",
    "that `hyperparams` is a static argument because it is made of Python types, not\n",
    "valid JAX types, and it also used interally for control flow. Read more about\n",
    "jit-compile [here](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tree_util.Partial(jax.jit, static_argnames=\"hyperparams\")\n",
    "def model_fwd(\n",
    "    x_batch: np.array,\n",
    "    trainables,\n",
    "    non_trainables,\n",
    "    hyperparams\n",
    "):\n",
    "    return series_fwd(x_batch, trainables, non_trainables, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function\n",
    "The loss function is also jit-compiled for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(\n",
    "    batched_preds: jnp.array,\n",
    "    batched_targets: np.array\n",
    "):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(\n",
    "        batched_preds,\n",
    "        batched_targets\n",
    "    ).mean() # Optax returns per-example loss, this returns the mean batch loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a convenience function that get model predictions on batched inputs,\n",
    "and calculates the loss against batched targets.\n",
    "\n",
    "It returns the batch loss and updated `non_trainables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    trainables,\n",
    "    non_trainables,\n",
    "    hyperparams,\n",
    "):\n",
    "    preds, non_trainables = model_fwd(\n",
    "        x_batch, trainables, non_trainables, hyperparams\n",
    "    )\n",
    "    return loss_fn(preds, y_batch), non_trainables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer using Optax\n",
    "We pass the `trainables` to `init` to initialze the optimizer state.\n",
    "\n",
    "We also define a function that takes in `trainables` gradients and an\n",
    "`optim_state`, and returns updates to be applied on `trainables` and a new\n",
    "`optim_state`.\n",
    "\n",
    "Note we used `jax.tree_util.Partial` to wrap the `optim_fn`. Doing so allows the\n",
    "function to be passed to jit-compiled functions, notably `train_step`.\n",
    "Read more about this [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.Partial.html?highlight=Partial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.sgd(1e-3, momentum=0.8)\n",
    "optim_state = optimizer.init(trainables)\n",
    "optim_fn = tree_util.Partial(optimizer.update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training step\n",
    "We  use JAX's `value_and_grad` to calculate the batch loss and\n",
    "gradients with respect to the `trainables`. Read more about JAX's autodiff\n",
    "[here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#taking-derivatives-with-grad).\n",
    "\n",
    "The batch loss is only used for logging, but the gradients are passed to\n",
    "`optim_fn` to get update gradients and a new `optim_state`.\n",
    "\n",
    "We apply the update gradient on the model weights.\n",
    "\n",
    "Finally, we return the batch loss, new `trainables`, and the new `optim_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tree_util.Partial(jax.jit, static_argnames=\"hyperparams\")\n",
    "def train_step(\n",
    "    x_batch: np.array, \n",
    "    y_batch: np.array,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state\n",
    "):\n",
    "    # Find batch loss and gradients\n",
    "    (loss, non_trainables), gradients = jax.value_and_grad(\n",
    "        model_loss,\n",
    "        argnums=2, # gradients wrt trainables (argument 2 of model_loss)\n",
    "        has_aux=True # non_trainables is auxiliary data, loss is the true ouput\n",
    "    )(x_batch, y_batch, trainables, non_trainables, hyperparams)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    gradients, optim_state = optim_fn(gradients, optim_state)\n",
    "\n",
    "    # Update model_weights with new gradients\n",
    "    trainables = optax.apply_updates(gradients, trainables)\n",
    "    return loss, trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    X_train, y_train,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state\n",
    "):\n",
    "    num_batches = len(X_train)\n",
    "    train_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_train[i], y_train[i]\n",
    "        loss, trainables, non_trainables, optim_state = train_step(\n",
    "            x_batch, y_batch,\n",
    "            trainables, non_trainables, hyperparams,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / num_batches}\") \n",
    "    return trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams\n",
    "):\n",
    "    num_batches = len(X_test)\n",
    "    test_loss, accuracy = 0, 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_test[i], y_test[i]\n",
    "        preds, _ = model_fwd(\n",
    "            x_batch, trainables, non_trainables, hyperparams\n",
    "        )\n",
    "        loss = loss_fn(preds, y_batch)\n",
    "        test_loss += loss\n",
    "        accuracy += (jnp.argmax(preds, axis=1) == y_batch).sum() / len(x_batch)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / num_batches}, accuracy: {accuracy / num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        trainables, non_trainables, optim_state = train_epoch(\n",
    "            X_train, y_train,\n",
    "            trainables, non_trainables, hyperparams,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(X_test, y_test, trainables, non_trainables, hyperparams)\n",
    "        print(f\"----------------\")\n",
    "    \n",
    "    return trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MLP on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 2.2807161808013916\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.1640022099018097\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.1046610102057457\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.07358527928590775\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.05379109084606171\n",
      "Test loss: 0.1475197821855545, accuracy: 0.962579607963562\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.04026977717876434\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.029762836173176765\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.022434595972299576\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.016575807705521584\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.012139208614826202\n",
      "Test loss: 0.15255959331989288, accuracy: 0.9662619829177856\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.009353498928248882\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.00700011383742094\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.0053950282745063305\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.0042128730565309525\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.003435919526964426\n",
      "Test loss: 0.1675001084804535, accuracy: 0.9671576619148254\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.002788830315694213\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.002337004989385605\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.0019783838652074337\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.0017167935147881508\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.0014933237107470632\n",
      "Test loss: 0.17379873991012573, accuracy: 0.9693471193313599\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.0013196150539442897\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.0011766041861847043\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.0010626270668581128\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.0009672010783106089\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.00088464719010517\n",
      "Test loss: 0.1777530312538147, accuracy: 0.9707404375076294\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.0008158026030287147\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.0007540366495959461\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.0007043257355690002\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.00065545120742172\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.0006174474256113172\n",
      "Test loss: 0.18021173775196075, accuracy: 0.9703423976898193\n",
      "----------------\n",
      "Epoch 31\n",
      "----------------\n",
      "Train loss: 0.0005812873714603484\n",
      "----------------\n",
      "Epoch 32\n",
      "----------------\n",
      "Train loss: 0.0005481453845277429\n",
      "----------------\n",
      "Epoch 33\n",
      "----------------\n",
      "Train loss: 0.0005185047630220652\n",
      "----------------\n",
      "Epoch 34\n",
      "----------------\n",
      "Train loss: 0.0004931706353090703\n",
      "----------------\n",
      "Epoch 35\n",
      "----------------\n",
      "Train loss: 0.00046857830602675676\n",
      "Test loss: 0.18253931403160095, accuracy: 0.9702428579330444\n",
      "----------------\n",
      "Epoch 36\n",
      "----------------\n",
      "Train loss: 0.0004474962188396603\n",
      "----------------\n",
      "Epoch 37\n",
      "----------------\n",
      "Train loss: 0.00042678663157857955\n",
      "----------------\n",
      "Epoch 38\n",
      "----------------\n",
      "Train loss: 0.0004087300621904433\n",
      "----------------\n",
      "Epoch 39\n",
      "----------------\n",
      "Train loss: 0.0003912621468771249\n",
      "----------------\n",
      "Epoch 40\n",
      "----------------\n",
      "Train loss: 0.0003758255625143647\n",
      "Test loss: 0.18458086252212524, accuracy: 0.9704418778419495\n",
      "----------------\n",
      "Epoch 41\n",
      "----------------\n",
      "Train loss: 0.0003605967212934047\n",
      "----------------\n",
      "Epoch 42\n",
      "----------------\n",
      "Train loss: 0.0003472836106084287\n",
      "----------------\n",
      "Epoch 43\n",
      "----------------\n",
      "Train loss: 0.000334816868416965\n",
      "----------------\n",
      "Epoch 44\n",
      "----------------\n",
      "Train loss: 0.00032299585291184485\n",
      "----------------\n",
      "Epoch 45\n",
      "----------------\n",
      "Train loss: 0.0003120153269264847\n",
      "Test loss: 0.18624994158744812, accuracy: 0.9708399772644043\n",
      "----------------\n",
      "Epoch 46\n",
      "----------------\n",
      "Train loss: 0.0003020324802491814\n",
      "----------------\n",
      "Epoch 47\n",
      "----------------\n",
      "Train loss: 0.0002921100240200758\n",
      "----------------\n",
      "Epoch 48\n",
      "----------------\n",
      "Train loss: 0.00028318201657384634\n",
      "----------------\n",
      "Epoch 49\n",
      "----------------\n",
      "Train loss: 0.000274582882411778\n",
      "----------------\n",
      "Epoch 50\n",
      "----------------\n",
      "Train loss: 0.00026650240761227906\n",
      "Test loss: 0.1877196580171585, accuracy: 0.9714370965957642\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_trainables, new_non_trainables, new_optim_state = train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state,\n",
    "    50, 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c2eae4f387f48c6f060ba94db73f9ed1c3f6b20f388481d7647fcca09c0c17a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
