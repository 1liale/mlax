{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation in mlax with Optax optimizers.\n",
    "This notebook uses the [Optax](https://optax.readthedocs.io/en/latest/optax-101.html) JAX optimization library.\n",
    "\n",
    "You can view the Pytorch reference implementation in `mlp_reference.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import (\n",
    "    nn,\n",
    "    random,\n",
    "    tree_util\n",
    ")\n",
    "import numpy as np\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import `Linear`, `Bias`, and `F` transformations from `mlax.nn` to build some\n",
    "dense layers.\n",
    "\n",
    "We import `series_fwd` from `mlax.block` to stack the dense layers into an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax.nn import Linear, Bias, F\n",
    "from mlax.block import series_fwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import helpers to load data from Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import batch, load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and batch the MNIST datasets.\n",
    "We use helper functions to load in Pytorch datasets as numpy and convert them in\n",
    "to lists containing the batches.\n",
    "\n",
    "Checkout\n",
    "[Training a Simple Neural Network with tensorflow/datasets Data Loading](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html) and\n",
    "[Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)\n",
    "for other ways to load in Tensorflow and Pytorch datsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n",
      "938 938\n",
      "157 157\n"
     ]
    }
   ],
   "source": [
    "# Load in datasets with helper\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(\"../data\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Batch datasets\n",
    "batch_size = 64\n",
    "X_train, y_train = batch(X_train, y_train, batch_size)\n",
    "X_test, y_test = batch(X_test, y_test, batch_size)\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLP model parameters.\n",
    "`model_init` consumes a `jax.random.PRNGKey` when initializing the parameters.\n",
    "Read more about random numbers in JAX [here](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html).\n",
    "\n",
    "`F` is a wrapper around a stateless function. `Linear` is a linear\n",
    "transformation without bias. `Bias` adds a bias term.\n",
    "\n",
    "Each `init` function returns a `trainables`, `non_trainables`, and\n",
    "`hyperparams`.\n",
    "\n",
    "`trainables` are trainable weights. `non_trainables` are non-trainable\n",
    "variables. `hyperparams` are additional parameters required by the forward pass.\n",
    "\n",
    "The `trainables` and `non_trainables` are PyTrees of JAX arrays. Read more about\n",
    "JAX PyTrees [here](https://jax.readthedocs.io/en/latest/pytrees.html).\n",
    "\n",
    "`hyperparams` is a NamedTuple containing Python types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(key):\n",
    "    keys_iter = iter(random.split(key, 6))\n",
    "    return zip(\n",
    "        # Convert int8 numpy inputs to float32 JAX arrays and flatten them\n",
    "        F.init(lambda x:jnp.reshape(\n",
    "            jnp.asarray(x, jnp.float32) / 256.0,\n",
    "            (len(x), -1))\n",
    "        ),\n",
    "\n",
    "        # Dense layer with relu activation\n",
    "        Linear.init(\n",
    "            key=next(keys_iter),\n",
    "            in_features=28 * 28,\n",
    "            out_features=512,\n",
    "        ),\n",
    "        Bias.init(\n",
    "            key=next(keys_iter),\n",
    "            in_feature_shape=(512,)\n",
    "        ),\n",
    "        F.init(nn.relu),\n",
    "\n",
    "        # Dense layer with relu activation\n",
    "        Linear.init(next(keys_iter), 512, 512),\n",
    "        Bias.init(next(keys_iter), (512,)),\n",
    "        F.init(nn.relu),\n",
    "        \n",
    "        # Dense layer without activation or softmax\n",
    "        Linear.init(next(keys_iter), 512, 10),\n",
    "        Bias.init(next(keys_iter), (10,)),\n",
    "    )\n",
    "\n",
    "trainables, non_trainables, hyperparams = model_init(random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MLP dataflow\n",
    "The `series_fwd` function takes in batched input features and tuples of\n",
    "`trainables`, `non_trainables`, and `hyperparams`. It figures out which layer\n",
    "each `hyperparams` is for, and calls their forward pass functions on the input\n",
    "features in sequence.\n",
    "\n",
    "It returns the model predictions and updated `non_trainables`.\n",
    "\n",
    "We jit-compile the forward pass function for significant speedups. Note\n",
    "that `hyperparams` is a static argument because it is made of Python types, not\n",
    "valid JAX types, and it also used interally for control flow. Read more about\n",
    "jit-compile [here](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tree_util.Partial(jax.jit, static_argnames=\"hyperparams\")\n",
    "def model_fwd(\n",
    "    x_batch: np.array,\n",
    "    trainables,\n",
    "    non_trainables,\n",
    "    hyperparams\n",
    "):\n",
    "    return series_fwd(x_batch, trainables, non_trainables, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function\n",
    "The loss function is also jit-compiled for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(\n",
    "    batched_preds: jnp.array,\n",
    "    batched_targets: np.array\n",
    "):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(\n",
    "        batched_preds,\n",
    "        batched_targets\n",
    "    ).mean() # Optax returns per-example loss, this returns the mean batch loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a convenience function that get model predictions on batched inputs,\n",
    "and calculates the loss against batched targets.\n",
    "\n",
    "It returns the batch loss and updated `non_trainables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    trainables,\n",
    "    non_trainables,\n",
    "    hyperparams,\n",
    "):\n",
    "    preds, non_trainables = model_fwd(\n",
    "        x_batch, trainables, non_trainables, hyperparams\n",
    "    )\n",
    "    return loss_fn(preds, y_batch), non_trainables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer using Optax\n",
    "We pass the `trainables` to `init` to initialize an optimizer state.\n",
    "\n",
    "We define a function that takes in `trainables` gradients and an `optim_state`,\n",
    "and returns updates to be applied on `trainables` and a new `optim_state`.\n",
    "\n",
    "Note we used `jax.tree_util.Partial` to wrap the `optim_fn`. Doing so allows the\n",
    "`optim_fn` to be passed to jit-compiled functions, notably `train_step`.\n",
    "Read more about this [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.Partial.html?highlight=Partial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.sgd(5e-3, momentum=0.6)\n",
    "optim_state = optimizer.init(trainables)\n",
    "optim_fn = tree_util.Partial(optimizer.update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training step\n",
    "We  use JAX's `value_and_grad` to calculate the batch loss and\n",
    "gradients with respect to the `trainables`. Read more about JAX's autodiff\n",
    "[here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#taking-derivatives-with-grad).\n",
    "\n",
    "The batch loss is only used for logging, but the gradients are passed to\n",
    "`optim_fn` to get update gradients and a new `optim_state`.\n",
    "\n",
    "We apply the update gradient on the model weights.\n",
    "\n",
    "Finally, we return the batch loss, new `trainables`, and the new `optim_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tree_util.Partial(jax.jit, static_argnames=\"hyperparams\")\n",
    "def train_step(\n",
    "    x_batch: np.array, \n",
    "    y_batch: np.array,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state\n",
    "):\n",
    "    # Find batch loss and gradients\n",
    "    (loss, non_trainables), gradients = jax.value_and_grad(\n",
    "        model_loss,\n",
    "        argnums=2, # gradients wrt trainables (argument 2 of model_loss)\n",
    "        has_aux=True # non_trainables is auxiliary data, loss is the true ouput\n",
    "    )(x_batch, y_batch, trainables, non_trainables, hyperparams)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    gradients, optim_state = optim_fn(gradients, optim_state)\n",
    "\n",
    "    # Update model_weights with new gradients\n",
    "    trainables = optax.apply_updates(gradients, trainables)\n",
    "    return loss, trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    X_train, y_train,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state\n",
    "):\n",
    "    num_batches = len(X_train)\n",
    "    train_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_train[i], y_train[i]\n",
    "        loss, trainables, non_trainables, optim_state = train_step(\n",
    "            x_batch, y_batch,\n",
    "            trainables, non_trainables, hyperparams,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / num_batches}\") \n",
    "    return trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams\n",
    "):\n",
    "    num_batches = len(X_test)\n",
    "    test_loss, accuracy = 0, 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_test[i], y_test[i]\n",
    "        preds, _ = model_fwd(\n",
    "            x_batch, trainables, non_trainables, hyperparams\n",
    "        )\n",
    "        loss = loss_fn(preds, y_batch)\n",
    "        test_loss += loss\n",
    "        accuracy += (jnp.argmax(preds, axis=1) == y_batch).sum() / len(x_batch)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / num_batches}, accuracy: {accuracy / num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        trainables, non_trainables, optim_state = train_epoch(\n",
    "            X_train, y_train,\n",
    "            trainables, non_trainables, hyperparams,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(X_test, y_test, trainables, non_trainables, hyperparams)\n",
    "        print(f\"----------------\")\n",
    "    \n",
    "    return trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MLP on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 0.7217015624046326\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.3205382227897644\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.26778843998908997\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.2356039434671402\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.21134009957313538\n",
      "Test loss: 0.20039354264736176, accuracy: 0.9424761533737183\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.19164066016674042\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.17506428062915802\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.1608925759792328\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.14859704673290253\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.13783679902553558\n",
      "Test loss: 0.13967624306678772, accuracy: 0.9586982727050781\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.12832310795783997\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.11983926594257355\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.11221276968717575\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.10531611740589142\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.09905103594064713\n",
      "Test loss: 0.1103348582983017, accuracy: 0.9678543210029602\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.09335420280694962\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.0881425142288208\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.08335482329130173\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.07894813269376755\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.07487884908914566\n",
      "Test loss: 0.09358730912208557, accuracy: 0.971835196018219\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.07112526148557663\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.0676451027393341\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.06440849602222443\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.06140260025858879\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.058589234948158264\n",
      "Test loss: 0.08338940888643265, accuracy: 0.974721372127533\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.055960092693567276\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.0534980446100235\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.05118183791637421\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.04899783805012703\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.04692664369940758\n",
      "Test loss: 0.07675064355134964, accuracy: 0.9766122698783875\n",
      "----------------\n",
      "Epoch 31\n",
      "----------------\n",
      "Train loss: 0.04496866464614868\n",
      "----------------\n",
      "Epoch 32\n",
      "----------------\n",
      "Train loss: 0.04311877861618996\n",
      "----------------\n",
      "Epoch 33\n",
      "----------------\n",
      "Train loss: 0.04136479273438454\n",
      "----------------\n",
      "Epoch 34\n",
      "----------------\n",
      "Train loss: 0.03969518467783928\n",
      "----------------\n",
      "Epoch 35\n",
      "----------------\n",
      "Train loss: 0.038107261061668396\n",
      "Test loss: 0.07233752310276031, accuracy: 0.9775079488754272\n",
      "----------------\n",
      "Epoch 36\n",
      "----------------\n",
      "Train loss: 0.036606565117836\n",
      "----------------\n",
      "Epoch 37\n",
      "----------------\n",
      "Train loss: 0.035172149538993835\n",
      "----------------\n",
      "Epoch 38\n",
      "----------------\n",
      "Train loss: 0.0338018499314785\n",
      "----------------\n",
      "Epoch 39\n",
      "----------------\n",
      "Train loss: 0.03249777480959892\n",
      "----------------\n",
      "Epoch 40\n",
      "----------------\n",
      "Train loss: 0.03126060962677002\n",
      "Test loss: 0.0693393275141716, accuracy: 0.9784036874771118\n",
      "----------------\n",
      "Epoch 41\n",
      "----------------\n",
      "Train loss: 0.030072879046201706\n",
      "----------------\n",
      "Epoch 42\n",
      "----------------\n",
      "Train loss: 0.028935717418789864\n",
      "----------------\n",
      "Epoch 43\n",
      "----------------\n",
      "Train loss: 0.02785029076039791\n",
      "----------------\n",
      "Epoch 44\n",
      "----------------\n",
      "Train loss: 0.026806972920894623\n",
      "----------------\n",
      "Epoch 45\n",
      "----------------\n",
      "Train loss: 0.025809234008193016\n",
      "Test loss: 0.0673505961894989, accuracy: 0.9794984459877014\n",
      "----------------\n",
      "Epoch 46\n",
      "----------------\n",
      "Train loss: 0.024856233969330788\n",
      "----------------\n",
      "Epoch 47\n",
      "----------------\n",
      "Train loss: 0.02394731342792511\n",
      "----------------\n",
      "Epoch 48\n",
      "----------------\n",
      "Train loss: 0.0230682585388422\n",
      "----------------\n",
      "Epoch 49\n",
      "----------------\n",
      "Train loss: 0.022235548123717308\n",
      "----------------\n",
      "Epoch 50\n",
      "----------------\n",
      "Train loss: 0.021434836089611053\n",
      "Test loss: 0.06599614024162292, accuracy: 0.9794984459877014\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_trainables, new_non_trainables, new_optim_state = train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state,\n",
    "    50, 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c2eae4f387f48c6f060ba94db73f9ed1c3f6b20f388481d7647fcca09c0c17a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
