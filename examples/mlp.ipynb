{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation in mlax.\n",
    "To help with understaning, consider first reading the tutorial on\n",
    "[Stateful Computations in JAX](https://jax.readthedocs.io/en/latest/jax-101/07-state.html),\n",
    "especially the Linear Regression worked example. \n",
    "\n",
    "Also consider going over the Pytorch reference implementation in\n",
    "`mlp_reference.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import nn\n",
    "from jax import random\n",
    "from jax import tree_util\n",
    "import torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the `Linear` block from `mlax.nn.blocks` to build the MLP.\n",
    "\n",
    "We import the `categorical_crossentropy` loss function from `mlax.losses` to\n",
    "evaluate our MLP.\n",
    "\n",
    "We import the `sgd` optimizer to from `mlax.optim` calculate updates on the\n",
    "model weights, and we import the `apply_gradient` function to apply those\n",
    "updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax.nn.blocks import Linear\n",
    "from mlax.losses import categorical_crossentropy\n",
    "from mlax.optim import sgd, apply_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset from torchvision\n",
    "Unlike in the reference implementation, `toTensor` transformation is not applied.\n",
    "This is because we will be converting the datasets to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True\n",
    ")\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the datasets into numpy arrays.\n",
    "\n",
    "We could instead convert the datasets to JAX arrays. If we did so, the dataset\n",
    "will be sent uncommitted to the default device, in my case, the GPU.\n",
    "Read more about this [here](https://jax.readthedocs.io/en/latest/faq.html#controlling-data-and-computation-placement-on-devices).\n",
    "\n",
    "Since most datasets are too expensive to copy to to each accelerator, they\n",
    "should stay on the CPU and be streamed to the accelerators during training and\n",
    "testing. While we could do this by using `jax.device_put()`, it's simpler to convert them to numpy arrays, which always stay on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = mnist_train.data.numpy(), mnist_train.targets.numpy()\n",
    "X_test, y_test = mnist_test.data.numpy(), mnist_test.targets.numpy()\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch the datasets\n",
    "Data stay as numpy arrays, and therefore on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 938\n",
      "157 157\n"
     ]
    }
   ],
   "source": [
    "def batch(x, y, batch_size, dtype=\"float32\"):\n",
    "  batched_x, batched_y = [], []\n",
    "  for i in range(0, len(x), batch_size):\n",
    "      batched_x.append(x[i:i+batch_size])\n",
    "      batched_y.append(y[i:i+batch_size])\n",
    "  return batched_x, batched_y\n",
    "\n",
    "batch_size = 64\n",
    "X_train, y_train = batch(X_train, y_train, batch_size)\n",
    "X_test, y_test = batch(X_test, y_test, batch_size)\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are more elegant ways to load in Tensorflow or Pytorch data.\n",
    "\n",
    "Consider reading Jax's\n",
    "[Training a Simple Neural Network with tensorflow/datasets Data Loading](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html) and\n",
    "[Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLP model weights.\n",
    "`model_init` uses a `jax.random.PRNGKey` when initializing the weights.\n",
    "Read more about random numbers in JAX [here](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html).\n",
    "\n",
    "The initialized `model_weights` are JAX arrays, so they reside on the default\n",
    "device, in my case the GPU.\n",
    "\n",
    "Furthermore, default weights of `mlax` transformations and blocks are of the type\n",
    "`float32`. You can override that with `init` functions' `dtype` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(key):\n",
    "    key1, key2, key3 = random.split(key, 3)\n",
    "    \n",
    "    # Initialize weights for each linear block on the GPU, default type: float32\n",
    "    w1 = Linear.init(key1, 28*28, 512)\n",
    "    w2 = Linear.init(key2, 512, 512)\n",
    "    w3 = Linear.init(key3, 512, 10)\n",
    "    return [w1, w2, w3]\n",
    "\n",
    "model_weights = model_init(random.PRNGKey(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MLP dataflow\n",
    "We first flatten the 2D numpy array with `jax.numpy.reshape`, converting it\n",
    "into a JAX array , which gets sent to the default device, in my case the GPU.\n",
    "\n",
    "We also explicitly convert its type to `float32`, the type of our\n",
    "model weights.\n",
    "\n",
    "Knowing what types we are working with is important because mlax functions do\n",
    "not promote types implicitly.\n",
    "\n",
    "We then pass the flattened inputs through several `mlax.nn.Linear` blocks, the\n",
    "last of which ends with a `jax.nn.softmax` activation.\n",
    "\n",
    "mlax functions, including `mlax.nn.Linear`, only accept a single unbatched\n",
    "sample as input. So `model_fwd` only works on single unbatched samples. So we\n",
    "use  JAX's [vmap](https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html)\n",
    "to define a `batched_model_fwd`, which works on batched inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def model_fwd(\n",
    "    x: np.array,\n",
    "    weights: jnp.array\n",
    "):\n",
    "    # Flatten numpy array and send it to GPU.\n",
    "    x = jnp.reshape(x, (-1, ))\n",
    "    # Explicit type promotion for the following mlax functions\n",
    "    x = x.astype(\"float32\") \n",
    "\n",
    "    w1, w2, w3 = weights\n",
    "    x = Linear.fwd(x, w1, nn.relu)\n",
    "    x = Linear.fwd(x, w2, nn.relu)\n",
    "    x = Linear.fwd(x, w3, nn.softmax)\n",
    "    return x\n",
    "\n",
    "batched_model_fwd = jax.vmap(model_fwd, in_axes=[0, None])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function\n",
    "`int8` numpy target values are first one-hot encoded and explicitly converted to\n",
    " a `float32` JAX array, which get sent to the default device, in my case the GPU.\n",
    "\n",
    "Predicted probabilities are clipped to remove `0`s, which will cause `NaN` loss\n",
    "when passed to the crossentropy function.\n",
    "\n",
    "Like all mlax functions, `mlax.losses.categorical_crossentropy` only works on a\n",
    "single unbatched sample. Again, we use JAX's `vmap` to change that.\n",
    "\n",
    "We also divide each loss by the number of classes to match Pytorch's behavior.\n",
    "\n",
    "The mean per-sample loss is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def batched_loss(\n",
    "    batched_preds: jnp.array,\n",
    "    batched_targets: np.array\n",
    "):\n",
    "    n_classes = 10\n",
    "    # One-hot encode numpy targets and send to GPU, promoting them to float32\n",
    "    batched_targets = nn.one_hot(batched_targets, n_classes, dtype=\"float32\")\n",
    "    # Clip predicted probabilities to remove 0s.\n",
    "    batched_preds = jnp.clip(batched_preds, 1e-7, 1 - 1e-7)\n",
    "\n",
    "    # Calculate per-sample loss\n",
    "    losses = jax.vmap(categorical_crossentropy)(\n",
    "        batched_preds, batched_targets\n",
    "    ) \n",
    "    # Match Pytorch behavior, optional\n",
    "    losses = losses / n_classes\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a convenience function that calculates model loss based on datasets' batched inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_model_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    weights: jnp.array\n",
    "):\n",
    "    return batched_loss(batched_model_fwd(x_batch, weights), y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer state and function\n",
    "As with model weights, the optimzer state is a JAX array that gets sent to the\n",
    "default device.\n",
    "\n",
    "During initialization, the optimizer infers the type of its state from the\n",
    "model weights. In our case then, the optimizer state is also of dtype `float32`.\n",
    "\n",
    "We create an optimizer function that takes in gradients and an optimizer state,\n",
    "and  returns new gradients to be applied and a new optimizer state.\n",
    "\n",
    "Note we used `jax.tree_util.Partial` instead of `functools.Partial` to allow\n",
    "using this function in `jit` compiled functions. Read more about this [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.Partial.html?highlight=Partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer state on the GPU\n",
    "optim_state = sgd.init(model_weights)\n",
    "optim_fn = tree_util.Partial(sgd.step, lr=1e-2, momentum=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training step\n",
    "We first calculate the batch loss, which shall be used for logging.\n",
    "\n",
    "We then use JAX's `grad` to calculate the\n",
    "gradients with repect to `model_weights`. Read more about JAX's autodiff\n",
    "[here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#taking-derivatives-with-grad).\n",
    "\n",
    "We use the `optim_fn` to get new gradients and a new optimizer state.\n",
    "\n",
    "We apply the new gradients to update the model weights.\n",
    "\n",
    "Finally, we return the batch loss, new `model_weights`, and new `optim_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(\n",
    "    x_batch: np.array, \n",
    "    y_batch: np.array,\n",
    "    model_weights: jnp.array,\n",
    "    optim_fn, # (gradients, optim_state) -> (new_gradients, new_optim_state)\n",
    "    optim_state: jnp.array\n",
    "):\n",
    "    # Find batch loss, only useful for logging\n",
    "    loss = batched_model_loss(x_batch, y_batch, model_weights)\n",
    "    # Find gradients wrt model_weights (argument 2)\n",
    "    gradients = jax.grad(batched_model_loss, argnums=2)(\n",
    "        x_batch,\n",
    "        y_batch,\n",
    "        model_weights\n",
    "    )\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    gradients, optim_state = optim_fn(gradients, optim_state)\n",
    "\n",
    "    # Update model_weights with new gradients\n",
    "    model_weights = apply_gradients(gradients, model_weights)\n",
    "    return loss, model_weights, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X_train, y_train, model_weights, optim_fn, optim_state):\n",
    "    num_batches = len(X_train)\n",
    "    train_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_train[i], y_train[i]\n",
    "        loss, model_weights, optim_state = train_step(\n",
    "            x_batch, y_batch,\n",
    "            model_weights,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / num_batches}\") \n",
    "    return model_weights, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, y_test, model_weights):\n",
    "    num_batches = len(X_test)\n",
    "    test_loss, accuracy = 0, 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_test[i], y_test[i]\n",
    "        preds = batched_model_fwd(\n",
    "            x_batch, model_weights\n",
    "        )\n",
    "        loss = batched_loss(preds, y_batch)\n",
    "        test_loss += loss\n",
    "        accuracy += (jnp.argmax(preds, axis=1) == y_batch).sum() / len(x_batch)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / num_batches}, accuracy: {accuracy / num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model_weights,\n",
    "    optim_fn, optim_state,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        model_weights, optim_state = train_epoch(\n",
    "            X_train, y_train,\n",
    "            model_weights,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(X_test, y_test, model_weights)\n",
    "        print(f\"----------------\")\n",
    "    \n",
    "    return model_weights, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MLP on MNIST dataset\n",
    "Achieves an accuracy of ~98%, similar to the Pytorch reference's.\n",
    "\n",
    "The mlax MLP does this in only 1 minutes 18 seconds as opposed to 4 minutes.\n",
    "\n",
    "Although this is an unfair comparison because we used `jax.jit` in strategic\n",
    "places to jit-compile functions using XLA. Read more about this\n",
    "[here](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 1.0663982629776\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 0.781200647354126\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 0.7016758322715759\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 0.6889051795005798\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.6860621571540833\n",
      "Test loss: 0.6802540421485901, accuracy: 0.5765326619148254\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.5907200574874878\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.5317518711090088\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.400712251663208\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.3803996741771698\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.3331351578235626\n",
      "Test loss: 0.2388990819454193, accuracy: 0.846337616443634\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.23317430913448334\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.22237780690193176\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.21365192532539368\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.20967940986156464\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.2070239633321762\n",
      "Test loss: 0.2146267294883728, accuracy: 0.8632563948631287\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.2012857049703598\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.19866801798343658\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.19686929881572723\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.19459226727485657\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.19219182431697845\n",
      "Test loss: 0.20152339339256287, accuracy: 0.8708201050758362\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.19042827188968658\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.18855948746204376\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.1870431751012802\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.18594437837600708\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.1844438761472702\n",
      "Test loss: 0.19350622594356537, accuracy: 0.8772889971733093\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.1839095801115036\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.13040299713611603\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.03022591769695282\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.02313949726521969\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.020843537524342537\n",
      "Test loss: 0.034693602472543716, accuracy: 0.9731289744377136\n",
      "----------------\n",
      "Epoch 31\n",
      "----------------\n",
      "Train loss: 0.017967745661735535\n",
      "----------------\n",
      "Epoch 32\n",
      "----------------\n",
      "Train loss: 0.016501614823937416\n",
      "----------------\n",
      "Epoch 33\n",
      "----------------\n",
      "Train loss: 0.014979627914726734\n",
      "----------------\n",
      "Epoch 34\n",
      "----------------\n",
      "Train loss: 0.01403609849512577\n",
      "----------------\n",
      "Epoch 35\n",
      "----------------\n",
      "Train loss: 0.012787184678018093\n",
      "Test loss: 0.0280446819961071, accuracy: 0.9784036874771118\n",
      "----------------\n",
      "Epoch 36\n",
      "----------------\n",
      "Train loss: 0.011091699823737144\n",
      "----------------\n",
      "Epoch 37\n",
      "----------------\n",
      "Train loss: 0.010921803303062916\n",
      "----------------\n",
      "Epoch 38\n",
      "----------------\n",
      "Train loss: 0.009544401429593563\n",
      "----------------\n",
      "Epoch 39\n",
      "----------------\n",
      "Train loss: 0.009381834417581558\n",
      "----------------\n",
      "Epoch 40\n",
      "----------------\n",
      "Train loss: 0.008315586484968662\n",
      "Test loss: 0.027364470064640045, accuracy: 0.9795979261398315\n",
      "----------------\n",
      "Epoch 41\n",
      "----------------\n",
      "Train loss: 0.007953032851219177\n",
      "----------------\n",
      "Epoch 42\n",
      "----------------\n",
      "Train loss: 0.007696730084717274\n",
      "----------------\n",
      "Epoch 43\n",
      "----------------\n",
      "Train loss: 0.007203243672847748\n",
      "----------------\n",
      "Epoch 44\n",
      "----------------\n",
      "Train loss: 0.007004628889262676\n",
      "----------------\n",
      "Epoch 45\n",
      "----------------\n",
      "Train loss: 0.0067861080169677734\n",
      "Test loss: 0.02573903277516365, accuracy: 0.9799960255622864\n",
      "----------------\n",
      "Epoch 46\n",
      "----------------\n",
      "Train loss: 0.006458956282585859\n",
      "----------------\n",
      "Epoch 47\n",
      "----------------\n",
      "Train loss: 0.006172632798552513\n",
      "----------------\n",
      "Epoch 48\n",
      "----------------\n",
      "Train loss: 0.006070761475712061\n",
      "----------------\n",
      "Epoch 49\n",
      "----------------\n",
      "Train loss: 0.005871529225260019\n",
      "----------------\n",
      "Epoch 50\n",
      "----------------\n",
      "Train loss: 0.005751835182309151\n",
      "Test loss: 0.025002583861351013, accuracy: 0.9805931448936462\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_model_weights, new_optim_state = train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model_weights,\n",
    "    optim_fn, optim_state,\n",
    "    50, 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "\n",
    "We first created a dataset that resides on the CPU.\n",
    "\n",
    "We then created a model whose weights reside on the GPU, but streams in\n",
    "samples from the CPU to work on. We made sure to explicitly convert the dtype of\n",
    "the streamed-in samples to match the dtype of model weights.\n",
    "\n",
    "We used `jax.vmap` on the model, so that it accepts batched inputs.\n",
    "\n",
    "We defined a loss function, which streams in targets from the CPU. We also made\n",
    "sure to explicitly convert the dtype of streamed-in targets to match the dtype\n",
    "of model predictions.\n",
    "\n",
    "We used `jax.vmap` on the loss function, so that it accepts batched predictions\n",
    "and targets.\n",
    "\n",
    "We then created an optimizer whose state is on the GPU, which operates on\n",
    "gradients also on the GPU.\n",
    "\n",
    "We used said optimizer to update the model weights, while also getting a new\n",
    "optimizer state.\n",
    "\n",
    "We repeated the predict-evaluate-optimize cycle with the new model weights and\n",
    "the new optimizer state over many batches until we get our desired model.\n",
    "\n",
    "Finally, we used `jax.jit` to compile each training step, which massively sped\n",
    "up our computations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c2eae4f387f48c6f060ba94db73f9ed1c3f6b20f388481d7647fcca09c0c17a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
