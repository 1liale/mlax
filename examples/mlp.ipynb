{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import nn\n",
    "from jax import random\n",
    "from jax import tree_util\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax.losses import categorical_crossentropy\n",
    "from mlax.nn.blocks import Linear \n",
    "from mlax.optim import apply, sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = jnp.asarray(mnist_train.data), jnp.asarray(mnist_train.targets)\n",
    "X_test, y_test = jnp.asarray(mnist_test.data), jnp.asarray(mnist_test.targets)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 938\n",
      "157 157\n"
     ]
    }
   ],
   "source": [
    "def batch(x, y, batch_size, k, dtype=\"float32\"):\n",
    "  x = x.astype(dtype)\n",
    "  y = y.astype(dtype) \n",
    "  batched_x, batched_y = [], []\n",
    "  for i in range(0, len(x), batch_size):\n",
    "      batched_x.append(x[i:i+batch_size])\n",
    "      batched_y.append(y[i:i+batch_size])\n",
    "  return batched_x, batched_y\n",
    "\n",
    "batch_size = 64\n",
    "X_train, y_train = batch(X_train, y_train, 64, 10)\n",
    "X_test, y_test = batch(X_test, y_test, 64, 10)\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(key):\n",
    "    key1, key2, key3 = random.split(key, 3)\n",
    "    w1 = Linear.init(key1, 28*28, 512)\n",
    "    w2 = Linear.init(key2, 512, 512)\n",
    "    w3 = Linear.init(key3, 512, 10)\n",
    "    return [w1, w2, w3]\n",
    "\n",
    "model_weights = model_init(random.PRNGKey(43))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fwd(x, weights):\n",
    "    x = jnp.reshape(x, (-1, ))\n",
    "    w1, w2, w3 = weights\n",
    "    x = Linear.fwd(x, w1, nn.relu)\n",
    "    x = Linear.fwd(x, w2, nn.relu)\n",
    "    x = Linear.fwd(x, w3, nn.softmax)\n",
    "    return x\n",
    "\n",
    "batched_model_fwd = jax.jit(jax.vmap(model_fwd, in_axes=[0, None]))\n",
    "\n",
    "@jax.jit\n",
    "def batched_loss(predictions, targets):\n",
    "    predictions = jnp.clip(predictions, 1e-9, 1 - 1e-9)\n",
    "    targets = nn.one_hot(targets, 10)\n",
    "    losses = jax.vmap(categorical_crossentropy)(\n",
    "        predictions, targets\n",
    "    )\n",
    "    return losses.mean()\n",
    "\n",
    "def batched_model_loss(x, y, weights):\n",
    "    return batched_loss(batched_model_fwd(x, weights), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_state = sgd.init(model_weights)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(x_batch, y_batch, model_weights, optim_state, lr):\n",
    "    loss = batched_model_loss(x_batch, y_batch, model_weights)\n",
    "    gradients = jax.grad(batched_model_loss, argnums=2)(\n",
    "        x_batch,\n",
    "        y_batch,\n",
    "        model_weights\n",
    "    )\n",
    "    gradients, optim_state = sgd.step(gradients, optim_state, lr, momentum=0.7)\n",
    "    model_weights = apply(gradients, model_weights)\n",
    "    return loss, model_weights, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X_train, y_train, model_weights, optim_state, lr):\n",
    "    num_batches = len(X_train)\n",
    "    train_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_train[i], y_train[i]\n",
    "        loss, model_weights, optim_state = train_step(\n",
    "            x_batch,\n",
    "            y_batch,\n",
    "            model_weights,\n",
    "            optim_state,\n",
    "            lr\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / num_batches}\") \n",
    "    return model_weights, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, y_test, model_weights):\n",
    "    num_batches = len(X_test)\n",
    "    test_loss, accuracy = 0, 0.0\n",
    "    for i in range(num_batches):\n",
    "        x_batch, y_batch = X_test[i], y_test[i]\n",
    "        preds = batched_model_fwd(\n",
    "            x_batch, model_weights\n",
    "        )\n",
    "        loss = batched_loss(preds, y_batch)\n",
    "        test_loss += loss\n",
    "        accuracy += (jnp.argmax(preds, axis=1) == y_batch).sum() / len(x_batch)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / num_batches}, accuracy: {accuracy / num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model_weights, optim_state, lr,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        model_weights, optim_state = train_epoch(\n",
    "            X_train, y_train,\n",
    "            model_weights, optim_state, lr\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(X_test, y_test, model_weights)\n",
    "        print(f\"----------------\")\n",
    "    \n",
    "    return model_weights, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 11.780515670776367\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 7.189499855041504\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 5.246441841125488\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 4.975132465362549\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 4.049658298492432\n",
      "Test loss: 2.898390531539917, accuracy: 0.8543989062309265\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 2.8120436668395996\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 2.6920533180236816\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 2.640648126602173\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 2.5953640937805176\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 1.625260353088379\n",
      "Test loss: 0.8335891962051392, accuracy: 0.9519307613372803\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.5922964215278625\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.46516886353492737\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.4051932990550995\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.33674362301826477\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.27836453914642334\n",
      "Test loss: 0.41886377334594727, accuracy: 0.9713375568389893\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.23747539520263672\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.20525266230106354\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.17285782098770142\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.1498226374387741\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.13074526190757751\n",
      "Test loss: 0.31942105293273926, accuracy: 0.9766122698783875\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.11246037483215332\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.09857557713985443\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.09053866565227509\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.08427561819553375\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.08368896692991257\n",
      "Test loss: 0.3105732202529907, accuracy: 0.9771098494529724\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.0813484713435173\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.07167407125234604\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.07074323296546936\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.06973830610513687\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.06912867724895477\n",
      "Test loss: 0.30197569727897644, accuracy: 0.9789012670516968\n",
      "----------------\n",
      "Epoch 31\n",
      "----------------\n",
      "Train loss: 0.06869913637638092\n",
      "----------------\n",
      "Epoch 32\n",
      "----------------\n",
      "Train loss: 0.06869783997535706\n",
      "----------------\n",
      "Epoch 33\n",
      "----------------\n",
      "Train loss: 0.0686974823474884\n",
      "----------------\n",
      "Epoch 34\n",
      "----------------\n",
      "Train loss: 0.06869727373123169\n",
      "----------------\n",
      "Epoch 35\n",
      "----------------\n",
      "Train loss: 0.06869711726903915\n",
      "Test loss: 0.30164650082588196, accuracy: 0.9789012670516968\n",
      "----------------\n",
      "Epoch 36\n",
      "----------------\n",
      "Train loss: 0.06869696825742722\n",
      "----------------\n",
      "Epoch 37\n",
      "----------------\n",
      "Train loss: 0.06869690120220184\n",
      "----------------\n",
      "Epoch 38\n",
      "----------------\n",
      "Train loss: 0.06869681924581528\n",
      "----------------\n",
      "Epoch 39\n",
      "----------------\n",
      "Train loss: 0.0686967596411705\n",
      "----------------\n",
      "Epoch 40\n",
      "----------------\n",
      "Train loss: 0.06869669258594513\n",
      "Test loss: 0.3015880882740021, accuracy: 0.9790008068084717\n",
      "----------------\n",
      "Epoch 41\n",
      "----------------\n",
      "Train loss: 0.06869664043188095\n",
      "----------------\n",
      "Epoch 42\n",
      "----------------\n",
      "Train loss: 0.06869658082723618\n",
      "----------------\n",
      "Epoch 43\n",
      "----------------\n",
      "Train loss: 0.06869654357433319\n",
      "----------------\n",
      "Epoch 44\n",
      "----------------\n",
      "Train loss: 0.0686965063214302\n",
      "----------------\n",
      "Epoch 45\n",
      "----------------\n",
      "Train loss: 0.06869647651910782\n",
      "Test loss: 0.3015492558479309, accuracy: 0.9790008068084717\n",
      "----------------\n",
      "Epoch 46\n",
      "----------------\n",
      "Train loss: 0.06869643926620483\n",
      "----------------\n",
      "Epoch 47\n",
      "----------------\n",
      "Train loss: 0.06869642436504364\n",
      "----------------\n",
      "Epoch 48\n",
      "----------------\n",
      "Train loss: 0.06869637966156006\n",
      "----------------\n",
      "Epoch 49\n",
      "----------------\n",
      "Train loss: 0.06869636476039886\n",
      "----------------\n",
      "Epoch 50\n",
      "----------------\n",
      "Train loss: 0.06869634240865707\n",
      "Test loss: 0.30150532722473145, accuracy: 0.9790008068084717\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_model_weights, new_optim_state = train_loop(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model_weights, optim_state,\n",
    "    8e-4,\n",
    "    50, 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ML': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "723f92442ed367d4370f31d60b5f82cb3c62acc21d19d96103bc7b04b520517d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
