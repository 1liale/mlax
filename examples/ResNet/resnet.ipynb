{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Implementation in mlax with Optax optimizers.\n",
    "This notebook uses the [Optax](https://optax.readthedocs.io/en/latest/optax-101.html) JAX optimization library.\n",
    "\n",
    "You can view the Pytorch reference implementation in `resnet_reference.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import (\n",
    "    nn,\n",
    "    random,\n",
    "    tree_util\n",
    ")\n",
    "import numpy as np\n",
    "import optax\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax.nn import Conv, Scaler, BatchNorm, Linear, Bias, F\n",
    "from mlax.block import Series, Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and batch the CIFAR-10 datasets.\n",
    "We follow this example\n",
    "[Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html) in using Pytorch dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "class ToNumpy(object):\n",
    "  def __call__(self, pic):\n",
    "    return np.array(pic)\n",
    "\n",
    "cifar_train = torchvision.datasets.CIFAR10(\n",
    "    root=\"../data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.AutoAugment(),\n",
    "    ToNumpy()\n",
    "])\n",
    ")\n",
    "cifar_test = torchvision.datasets.CIFAR10(\n",
    "    root=\"../data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToNumpy()\n",
    ")\n",
    "print(cifar_train.data.shape)\n",
    "print(cifar_test.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391 79\n"
     ]
    }
   ],
   "source": [
    "def numpy_collate(batch):\n",
    "  if isinstance(batch[0], np.ndarray):\n",
    "    return np.stack(batch)\n",
    "  elif isinstance(batch[0], (tuple,list)):\n",
    "    transposed = zip(*batch)\n",
    "    return [numpy_collate(samples) for samples in transposed]\n",
    "  else:\n",
    "    return np.array(batch)\n",
    "\n",
    "batch_size=128\n",
    "train_dataloader = DataLoader(\n",
    "    cifar_train, batch_size=128, shuffle=True, collate_fn=numpy_collate, num_workers=8\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    cifar_test, batch_size=128, shuffle=True, collate_fn=numpy_collate, num_workers=8\n",
    ")\n",
    "print(len(train_dataloader), len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ResNet model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def split(x):\n",
    "    return x, x\n",
    "\n",
    "def add(pair):\n",
    "    x, y = pair\n",
    "    return jax.lax.add(x, y)\n",
    "\n",
    "def res_block1_init(key, size):\n",
    "    keys_iter = iter(random.split(key, 8))\n",
    "    return Series.init(\n",
    "        F.init(split),\n",
    "        Parallel.init(\n",
    "            F.init(bypass),\n",
    "            Series.init(\n",
    "                # Conv with batchnorm\n",
    "                Conv.init(next(keys_iter), 2, size, size, 3, padding=1, channel_last=True),\n",
    "                BatchNorm.init(next(keys_iter), size, channel_axis=-1),\n",
    "                Scaler.init(next(keys_iter), (None, None, size)),\n",
    "                Bias.init(next(keys_iter), (None, None, size)),\n",
    "                F.init(nn.relu),\n",
    "                # Conv with batchnorm\n",
    "                Conv.init(next(keys_iter), 2, size, size, 3, padding=1, channel_last=True),\n",
    "                BatchNorm.init(next(keys_iter), size, channel_axis=-1),\n",
    "                Scaler.init(next(keys_iter), (None, None, size)),\n",
    "                Bias.init(next(keys_iter), (None, None, size)),\n",
    "                F.init(nn.relu),\n",
    "            )\n",
    "        ),\n",
    "        F.init(add)\n",
    "    )\n",
    "\n",
    "def res_block2_init(key, size):\n",
    "    keys_iter = iter(random.split(key, 12))\n",
    "    size2 = size * 2\n",
    "    return Series.init(\n",
    "        F.init(split),\n",
    "        Parallel.init(\n",
    "            # Downsampling conv with batchnorm\n",
    "            Series.init(\n",
    "                Conv.init(\n",
    "                    next(keys_iter), 2, size, size2, 3, strides=2, padding=1, channel_last=True\n",
    "                ),\n",
    "                BatchNorm.init(next(keys_iter), size2, channel_axis=-1),\n",
    "                Scaler.init(next(keys_iter), (None, None, size2)),\n",
    "                Bias.init(next(keys_iter), (None, None, size2)),\n",
    "                F.init(nn.relu),\n",
    "            ),\n",
    "            Series.init(\n",
    "                # Downsampling conv with batchnorm\n",
    "                Conv.init(\n",
    "                    next(keys_iter), 2, size, size2, 3, strides=2, padding=1, channel_last=True\n",
    "                ),\n",
    "                BatchNorm.init(next(keys_iter), size2, channel_axis=-1),\n",
    "                Scaler.init(next(keys_iter), (None, None, size2)),\n",
    "                Bias.init(next(keys_iter), (None, None, size2)),\n",
    "                F.init(nn.relu),\n",
    "                # Conv with batchnorm\n",
    "                Conv.init(next(keys_iter), 2, size2, size2, 3, padding=1, channel_last=True),\n",
    "                BatchNorm.init(next(keys_iter), size2, channel_axis=-1),\n",
    "                Scaler.init(next(keys_iter), (None, None, size2)),\n",
    "                Bias.init(next(keys_iter), (None, None, size2)),\n",
    "                F.init(nn.relu),\n",
    "            )\n",
    "        ),\n",
    "        F.init(add)\n",
    "    )\n",
    "\n",
    "def model_init(key):\n",
    "    keys_iter = iter(random.split(key, 9))\n",
    "    return Series.init(\n",
    "        # Convert int8 numpy inputs to float32 JAX arrays\n",
    "        F.init(\n",
    "            lambda x: jnp.asarray(x, jnp.float32) / 256.0,    \n",
    "        ),\n",
    "        # (N, 32, 32, 3)\n",
    "        Conv.init(next(keys_iter), 2, 3, 16, 3, padding=1, channel_last=True),\n",
    "        BatchNorm.init(next(keys_iter), 16, channel_axis=-1),\n",
    "        Scaler.init(next(keys_iter), (None, None, 16)),\n",
    "        Bias.init(next(keys_iter), (None, None, 16)),\n",
    "        F.init(nn.relu),\n",
    "        # (N, 32, 32, 16)\n",
    "        res_block1_init(next(keys_iter), 16),\n",
    "        # (N, 32, 32, 16)\n",
    "        res_block2_init(next(keys_iter), 16),\n",
    "        # (N, 16, 16, 32)\n",
    "        res_block2_init(next(keys_iter), 32),\n",
    "        # (N, 8, 8, 64)\n",
    "        F.init(lambda x: jnp.reshape(x.mean((1, 2)), (-1, 64))),\n",
    "        # (N, 64)\n",
    "        Linear.init(next(keys_iter), 64, 10),\n",
    "        # (N, 10)\n",
    "        Bias.init(next(keys_iter), (10,))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainables, non_trainables, hyperparams = model_init(random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ResNet dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fwd = jax.jit(\n",
    "    Series.fwd,\n",
    "    static_argnames=[\"hyperparams\", \"inference_mode\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(\n",
    "    batched_preds: jnp.array,\n",
    "    batched_targets: np.array\n",
    "):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(\n",
    "        batched_preds,\n",
    "        batched_targets\n",
    "    ).mean() # Optax returns per-example loss, this returns the mean batch loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a convenience function that get model predictions on batched inputs,\n",
    "and calculates the loss against batched targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    trainables,\n",
    "    non_trainables,\n",
    "    hyperparams,\n",
    "):\n",
    "    preds, non_trainables = model_fwd(\n",
    "        x_batch, trainables, non_trainables, hyperparams\n",
    "    )\n",
    "    return loss_fn(preds, y_batch), non_trainables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer using Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(1e-2)\n",
    "optim_state = optimizer.init(trainables)\n",
    "optim_fn = tree_util.Partial(optimizer.update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tree_util.Partial(jax.jit, static_argnames=\"hyperparams\")\n",
    "def train_step(\n",
    "    x_batch: np.array, \n",
    "    y_batch: np.array,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state\n",
    "):\n",
    "    # Find batch loss and gradients\n",
    "    (loss, non_trainables), gradients = jax.value_and_grad(\n",
    "        model_loss,\n",
    "        argnums=2, # gradients wrt trainables (argument 2 of model_loss)\n",
    "        has_aux=True # non_trainables is auxiliary data, loss is the true ouput\n",
    "    )(x_batch, y_batch, trainables, non_trainables, hyperparams)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    gradients, optim_state = optim_fn(gradients, optim_state)\n",
    "\n",
    "    # Update model_weights with new gradients\n",
    "    trainables = optax.apply_updates(gradients, trainables)\n",
    "    return loss, trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for training and testing loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    dataloader,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state\n",
    "):\n",
    "    train_loss = 0.0\n",
    "    for X, y in dataloader:\n",
    "        loss, trainables, non_trainables, optim_state = train_step(\n",
    "            X, y,\n",
    "            trainables, non_trainables, hyperparams,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / len(dataloader)}\") \n",
    "    return trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    dataloader,\n",
    "    trainables, non_trainables, hyperparams\n",
    "):\n",
    "    test_loss, accuracy = 0, 0.0\n",
    "    for X, y in dataloader:\n",
    "        preds, _ = model_fwd(\n",
    "            X, trainables, non_trainables, hyperparams\n",
    "        )\n",
    "        loss = loss_fn(preds, y)\n",
    "        test_loss += loss\n",
    "        accuracy += (jnp.argmax(preds, axis=1) == y).sum()\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / len(dataloader)}, accuracy: {accuracy / len(dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    train_dataloader, test_dataloader, \n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        trainables, non_trainables, optim_state = train_epoch(\n",
    "            train_dataloader,\n",
    "            trainables, non_trainables, hyperparams,\n",
    "            optim_fn, optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(test_dataloader, trainables, non_trainables, hyperparams)\n",
    "        print(f\"----------------\")\n",
    "    \n",
    "    return trainables, non_trainables, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 1.7923946380615234\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 1.3324811458587646\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 1.159685492515564\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 1.0457003116607666\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.9666723608970642\n",
      "Test loss: 0.8049401640892029, accuracy: 0.7178000211715698\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.9237094521522522\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.8859540224075317\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.8476534485816956\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.8247234225273132\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.7974581122398376\n",
      "Test loss: 0.6401292085647583, accuracy: 0.7791000604629517\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.7718923091888428\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.7550898194313049\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.7308756709098816\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.7230141758918762\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.7057121992111206\n",
      "Test loss: 0.61341392993927, accuracy: 0.7910000085830688\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.6966926455497742\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.6863746047019958\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.675020158290863\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.6626375317573547\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.6452587246894836\n",
      "Test loss: 0.5887557864189148, accuracy: 0.8032000660896301\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.635249137878418\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.6285514831542969\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.6161106824874878\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.6134726405143738\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.6071732640266418\n",
      "Test loss: 0.5659878849983215, accuracy: 0.8043000102043152\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.5958789587020874\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.5891380310058594\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.582734227180481\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.5783699750900269\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.5696518421173096\n",
      "Test loss: 0.5574473738670349, accuracy: 0.8114000558853149\n",
      "----------------\n",
      "Epoch 31\n",
      "----------------\n",
      "Train loss: 0.5658735036849976\n",
      "----------------\n",
      "Epoch 32\n",
      "----------------\n",
      "Train loss: 0.5584619045257568\n",
      "----------------\n",
      "Epoch 33\n",
      "----------------\n",
      "Train loss: 0.5505221486091614\n",
      "----------------\n",
      "Epoch 34\n",
      "----------------\n",
      "Train loss: 0.5419824719429016\n",
      "----------------\n",
      "Epoch 35\n",
      "----------------\n",
      "Train loss: 0.5421331524848938\n",
      "Test loss: 0.5690008401870728, accuracy: 0.8074000477790833\n",
      "----------------\n",
      "Epoch 36\n",
      "----------------\n",
      "Train loss: 0.5278365015983582\n",
      "----------------\n",
      "Epoch 37\n",
      "----------------\n",
      "Train loss: 0.5310394167900085\n",
      "----------------\n",
      "Epoch 38\n",
      "----------------\n",
      "Train loss: 0.5255162715911865\n",
      "----------------\n",
      "Epoch 39\n",
      "----------------\n",
      "Train loss: 0.5209967494010925\n",
      "----------------\n",
      "Epoch 40\n",
      "----------------\n",
      "Train loss: 0.5139298439025879\n",
      "Test loss: 0.5369501113891602, accuracy: 0.8201000094413757\n",
      "----------------\n",
      "Epoch 41\n",
      "----------------\n",
      "Train loss: 0.5086386799812317\n",
      "----------------\n",
      "Epoch 42\n",
      "----------------\n",
      "Train loss: 0.5075569748878479\n",
      "----------------\n",
      "Epoch 43\n",
      "----------------\n",
      "Train loss: 0.49805065989494324\n",
      "----------------\n",
      "Epoch 44\n",
      "----------------\n",
      "Train loss: 0.49586692452430725\n",
      "----------------\n",
      "Epoch 45\n",
      "----------------\n",
      "Train loss: 0.4970320761203766\n",
      "Test loss: 0.5458859205245972, accuracy: 0.8170000314712524\n",
      "----------------\n",
      "Epoch 46\n",
      "----------------\n",
      "Train loss: 0.49074068665504456\n",
      "----------------\n",
      "Epoch 47\n",
      "----------------\n",
      "Train loss: 0.48762547969818115\n",
      "----------------\n",
      "Epoch 48\n",
      "----------------\n",
      "Train loss: 0.48592111468315125\n",
      "----------------\n",
      "Epoch 49\n",
      "----------------\n",
      "Train loss: 0.4723866581916809\n",
      "----------------\n",
      "Epoch 50\n",
      "----------------\n",
      "Train loss: 0.4782320559024811\n",
      "Test loss: 0.5610348582267761, accuracy: 0.8147000670433044\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_trainables, new_non_trainables, new_optim_state = train_loop(\n",
    "    train_dataloader, test_dataloader,\n",
    "    trainables, non_trainables, hyperparams,\n",
    "    optim_fn, optim_state,\n",
    "    50, 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c2eae4f387f48c6f060ba94db73f9ed1c3f6b20f388481d7647fcca09c0c17a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
