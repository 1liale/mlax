{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Implementation in mlax with Optax optimizers.\n",
    "This notebook uses the [Optax](https://optax.readthedocs.io/en/latest/optax-101.html) JAX optimization library.\n",
    "\n",
    "You can view a full-precision implementation in `resnet.ipynb`.\n",
    "\n",
    "You can view the Pytorch reference implementation in `resnet_reference.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import (\n",
    "    numpy as jnp,\n",
    "    tree_util as jtu,\n",
    "    nn,\n",
    "    random,\n",
    "    lax\n",
    ")\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import optax\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlax import Module, is_trainable\n",
    "from mlax.nn import (\n",
    "    Conv, Scaler, BatchNorm, Linear, Bias, F, Series\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and batch the CIFAR-10 datasets.\n",
    "We follow this example\n",
    "[Training a Simple Neural Network, with PyTorch Data Loading](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html) in using Pytorch dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "class ToNumpy(object):\n",
    "  def __call__(self, pic):\n",
    "    return np.array(pic)\n",
    "\n",
    "cifar_train = torchvision.datasets.CIFAR10(\n",
    "    root=\"../data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.AutoAugment(),\n",
    "    ToNumpy()\n",
    "])\n",
    ")\n",
    "cifar_test = torchvision.datasets.CIFAR10(\n",
    "    root=\"../data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToNumpy()\n",
    ")\n",
    "print(cifar_train.data.shape)\n",
    "print(cifar_test.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391 79\n"
     ]
    }
   ],
   "source": [
    "def numpy_collate(batch):\n",
    "  if isinstance(batch[0], np.ndarray):\n",
    "    return np.stack(batch)\n",
    "  elif isinstance(batch[0], (tuple,list)):\n",
    "    transposed = zip(*batch)\n",
    "    return [numpy_collate(samples) for samples in transposed]\n",
    "  else:\n",
    "    return np.array(batch)\n",
    "\n",
    "batch_size=128\n",
    "train_dataloader = DataLoader(\n",
    "    cifar_train, batch_size, shuffle=True, collate_fn=numpy_collate, num_workers=6\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    cifar_test, batch_size, shuffle=True, collate_fn=numpy_collate, num_workers=6\n",
    ")\n",
    "print(len(train_dataloader), len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ResNet model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 10)\n"
     ]
    }
   ],
   "source": [
    "# 3x3 channel-last conv block with batchnorm\n",
    "class ConvBlock(Module):\n",
    "    def __init__(self, rng, out_channels, strides=1, batch_axis_name=\"batch\"):\n",
    "        super().__init__()\n",
    "        rngs_iter = iter(random.split(rng, 4))\n",
    "        self.conv = Conv(\n",
    "            next(rngs_iter), 2, out_channels, 3, strides, padding=1, channel_last=True\n",
    "        )\n",
    "        self.batchnorm = Series([\n",
    "            BatchNorm(next(rngs_iter), batch_axis_name, channel_last=True),\n",
    "            Scaler(next(rngs_iter), (None, None, out_channels)),\n",
    "            Bias(next(rngs_iter), (None, None, out_channels)),\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, x, rng=None, inference_mode=False):\n",
    "        x, self.conv = self.conv(x, None, inference_mode)\n",
    "        x_dtype = x.dtype\n",
    "        x = x.astype(jnp.float32) # Keep batchnorm in full precision\n",
    "        x, self.batchnorm = self.batchnorm(x, None, inference_mode)\n",
    "        x = x.astype(x_dtype)\n",
    "        return nn.relu(x), self\n",
    "\n",
    "# Residual block without downsampling (H, W, C) -> (H, W, C)\n",
    "class ResBlock1(Module):\n",
    "    def __init__(self, rng, out_channels):\n",
    "        super().__init__()\n",
    "        rng1, rng2 = random.split(rng)\n",
    "        self.block = Series([\n",
    "            ConvBlock(rng1, out_channels),\n",
    "            ConvBlock(rng2, out_channels)\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, x, rng=None, inference_mode=False):\n",
    "        acts, self.block = self.block(x, None, inference_mode)\n",
    "        return lax.add(acts, x), self\n",
    "\n",
    "# Residual block with downsampling (H, W, C) -> (H // 2, W // 2 2 * C)\n",
    "class ResBlock2(Module):\n",
    "    def __init__(self, rng, out_channels):\n",
    "        super().__init__()\n",
    "        rng1, rng2, rng3 = random.split(rng, 3)\n",
    "        self.block = Series([\n",
    "            ConvBlock(rng1, out_channels, strides=2),\n",
    "            ConvBlock(rng2, out_channels)\n",
    "        ])\n",
    "        self.downsample = ConvBlock(rng3, out_channels, strides=2)\n",
    "    \n",
    "    def __call__(self, x, rng=None, inference_mode=False):\n",
    "        acts, self.block = self.block(x, None, inference_mode)\n",
    "        x, self.downsample = self.downsample(x, None, inference_mode)\n",
    "        return lax.add(acts, x), self\n",
    "\n",
    "class ResNet(Module):\n",
    "    def __init__(self, rng):\n",
    "        super().__init__()\n",
    "        rngs_iter = iter(random.split(rng, 6))\n",
    "        self.conv = ConvBlock(next(rngs_iter), 16)\n",
    "        self.res1 = ResBlock1(next(rngs_iter), 16)\n",
    "        self.res2 = ResBlock2(next(rngs_iter), 32)\n",
    "        self.res3 = ResBlock2(next(rngs_iter), 64)\n",
    "        self.fc = Series([\n",
    "            Linear(next(rngs_iter), 10),\n",
    "            Bias(next(rngs_iter), (10,))\n",
    "        ])\n",
    "\n",
    "    @partial(\n",
    "        jax.vmap,\n",
    "        in_axes = (None, 0, None, None),\n",
    "        out_axes = (0, None),\n",
    "        axis_name = \"batch\"\n",
    "    ) # Add leading batch dimension\n",
    "    def __call__(self, x, rng=None, inference_mode=False):\n",
    "        x = x.astype(jnp.float16) / 255.0\n",
    "        # (32, 32, 3)\n",
    "        x, self.conv = self.conv(x, None, inference_mode)\n",
    "        # (32, 32, 16)\n",
    "        x, self.res1 = self.res1(x, None, inference_mode)\n",
    "        # (32, 32, 16)\n",
    "        x, self.res2 = self.res2(x, None, inference_mode)\n",
    "        # (16, 16, 32)\n",
    "        x, self.res3 = self.res3(x, None, inference_mode)\n",
    "        # (8, 8, 64)\n",
    "        x = jnp.reshape(x.mean((0, 1)), (-1,))\n",
    "        # (64,)\n",
    "        x, self.fc = self.fc(x, None, inference_mode)\n",
    "        # (10,)\n",
    "        return x, self\n",
    "\n",
    "model = ResNet(random.PRNGKey(0))\n",
    "\n",
    "# Induce lazy weight initialization\n",
    "for x_batch, y_batch in train_dataloader:\n",
    "    acts, model = model(x_batch, None, False)\n",
    "    print(acts.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    batched_preds: jnp.array,\n",
    "    batched_targets: np.array\n",
    "):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(\n",
    "        batched_preds,\n",
    "        batched_targets\n",
    "    ).mean() # Optax returns per-example loss, this returns the mean batch loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two convenience functions.\n",
    "\n",
    "``model_training_loss`` returns the batch loss and updated `model` from batched\n",
    "inputs and targets.\n",
    "\n",
    "``model_inference_preds_loss`` returns the batch loss and predictions from\n",
    "batched inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    trainables,\n",
    "    non_trainables,\n",
    "    scaling_factor = 1\n",
    "):\n",
    "    model = trainables.combine(non_trainables)\n",
    "    preds, model = model(x_batch, None, False)\n",
    "    return loss_fn(preds, y_batch) * scaling_factor, model\n",
    "\n",
    "@jax.jit\n",
    "def model_inference_loss(\n",
    "    x_batch: np.array,\n",
    "    y_batch: np.array,\n",
    "    model\n",
    "):\n",
    "    preds, _ = model(x_batch, None, True)\n",
    "    return loss_fn(preds, y_batch), preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer using Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(1e-2)\n",
    "optim_state = optimizer.init(model.filter(is_trainable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(\n",
    "    x_batch: np.array, \n",
    "    y_batch: np.array,\n",
    "    model,\n",
    "    optim_state\n",
    "):\n",
    "    scaling_factor = 2 ** 12\n",
    "\n",
    "    # Find batch loss and gradients with resect to trainables\n",
    "    (loss, model), gradients = jax.value_and_grad(\n",
    "        model_training_loss,\n",
    "        argnums=2, # gradients wrt trainables (argument 2 of model_training_loss)\n",
    "        has_aux=True # model is auxiliary data, loss is the true ouput\n",
    "    )(x_batch, y_batch, *model.partition(), scaling_factor)\n",
    "    \n",
    "    # Loss unscaling\n",
    "    loss = loss / scaling_factor\n",
    "    def unscale_gradients(x):\n",
    "        return x / scaling_factor\n",
    "    gradients = jtu.tree_map(unscale_gradients, gradients)\n",
    "\n",
    "    # Get new gradients and optimizer state\n",
    "    gradients, optim_state = optimizer.update(gradients, optim_state)\n",
    "\n",
    "    # Update model_weights with new gradients\n",
    "    trainables, non_trainables = model.partition()\n",
    "    trainables = optax.apply_updates(gradients, trainables)\n",
    "    return loss, trainables.combine(non_trainables), optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for training and testing loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    dataloader,\n",
    "    model,\n",
    "    optim_state\n",
    "):\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss = 0.0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        loss, model, optim_state = train_step(\n",
    "            x_batch, y_batch,\n",
    "            model,\n",
    "            optim_state\n",
    "        )\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f\"Train loss: {train_loss / num_batches}\") \n",
    "    return model, optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    dataloader,\n",
    "    model\n",
    "):\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, accuracy = 0, 0.0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        loss, preds = model_inference_loss(\n",
    "            x_batch, y_batch, model\n",
    "        )\n",
    "        test_loss += loss\n",
    "        accuracy += (jnp.argmax(preds, axis=1) == y_batch).sum() / len(x_batch)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / num_batches}, accuracy: {accuracy / num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    model,\n",
    "    optim_state,\n",
    "    epochs, test_every\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        epoch = i + 1\n",
    "        print(f\"Epoch {epoch}\\n----------------\")\n",
    "        model, optim_state = train_epoch(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            optim_state\n",
    "        )\n",
    "        if (epoch % test_every == 0):\n",
    "            test(test_dataloader, model)\n",
    "        print(f\"----------------\")\n",
    "    \n",
    "    return model, optim_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------\n",
      "Train loss: 1.8193359375\n",
      "----------------\n",
      "Epoch 2\n",
      "----------------\n",
      "Train loss: 1.3955078125\n",
      "----------------\n",
      "Epoch 3\n",
      "----------------\n",
      "Train loss: 1.19140625\n",
      "----------------\n",
      "Epoch 4\n",
      "----------------\n",
      "Train loss: 1.0732421875\n",
      "----------------\n",
      "Epoch 5\n",
      "----------------\n",
      "Train loss: 0.99755859375\n",
      "Test loss: 0.78076171875, accuracy: 0.7212223410606384\n",
      "----------------\n",
      "Epoch 6\n",
      "----------------\n",
      "Train loss: 0.93603515625\n",
      "----------------\n",
      "Epoch 7\n",
      "----------------\n",
      "Train loss: 0.89111328125\n",
      "----------------\n",
      "Epoch 8\n",
      "----------------\n",
      "Train loss: 0.86181640625\n",
      "----------------\n",
      "Epoch 9\n",
      "----------------\n",
      "Train loss: 0.82177734375\n",
      "----------------\n",
      "Epoch 10\n",
      "----------------\n",
      "Train loss: 0.79345703125\n",
      "Test loss: 0.896484375, accuracy: 0.6991693377494812\n",
      "----------------\n",
      "Epoch 11\n",
      "----------------\n",
      "Train loss: 0.78564453125\n",
      "----------------\n",
      "Epoch 12\n",
      "----------------\n",
      "Train loss: 0.7607421875\n",
      "----------------\n",
      "Epoch 13\n",
      "----------------\n",
      "Train loss: 0.7421875\n",
      "----------------\n",
      "Epoch 14\n",
      "----------------\n",
      "Train loss: 0.7236328125\n",
      "----------------\n",
      "Epoch 15\n",
      "----------------\n",
      "Train loss: 0.70068359375\n",
      "Test loss: 0.6904296875, accuracy: 0.7619659900665283\n",
      "----------------\n",
      "Epoch 16\n",
      "----------------\n",
      "Train loss: 0.69677734375\n",
      "----------------\n",
      "Epoch 17\n",
      "----------------\n",
      "Train loss: 0.677734375\n",
      "----------------\n",
      "Epoch 18\n",
      "----------------\n",
      "Train loss: 0.67578125\n",
      "----------------\n",
      "Epoch 19\n",
      "----------------\n",
      "Train loss: 0.65771484375\n",
      "----------------\n",
      "Epoch 20\n",
      "----------------\n",
      "Train loss: 0.650390625\n",
      "Test loss: 0.6171875, accuracy: 0.7921282052993774\n",
      "----------------\n",
      "Epoch 21\n",
      "----------------\n",
      "Train loss: 0.64013671875\n",
      "----------------\n",
      "Epoch 22\n",
      "----------------\n",
      "Train loss: 0.62548828125\n",
      "----------------\n",
      "Epoch 23\n",
      "----------------\n",
      "Train loss: 0.62255859375\n",
      "----------------\n",
      "Epoch 24\n",
      "----------------\n",
      "Train loss: 0.6083984375\n",
      "----------------\n",
      "Epoch 25\n",
      "----------------\n",
      "Train loss: 0.5947265625\n",
      "Test loss: 0.5859375, accuracy: 0.8035997152328491\n",
      "----------------\n",
      "Epoch 26\n",
      "----------------\n",
      "Train loss: 0.59765625\n",
      "----------------\n",
      "Epoch 27\n",
      "----------------\n",
      "Train loss: 0.587890625\n",
      "----------------\n",
      "Epoch 28\n",
      "----------------\n",
      "Train loss: 0.5771484375\n",
      "----------------\n",
      "Epoch 29\n",
      "----------------\n",
      "Train loss: 0.580078125\n",
      "----------------\n",
      "Epoch 30\n",
      "----------------\n",
      "Train loss: 0.56298828125\n",
      "Test loss: 0.57763671875, accuracy: 0.805874228477478\n",
      "----------------\n",
      "Epoch 31\n",
      "----------------\n",
      "Train loss: 0.56298828125\n",
      "----------------\n",
      "Epoch 32\n",
      "----------------\n",
      "Train loss: 0.5595703125\n",
      "----------------\n",
      "Epoch 33\n",
      "----------------\n",
      "Train loss: 0.544921875\n",
      "----------------\n",
      "Epoch 34\n",
      "----------------\n",
      "Train loss: 0.5419921875\n",
      "----------------\n",
      "Epoch 35\n",
      "----------------\n",
      "Train loss: 0.54296875\n",
      "Test loss: 0.5712890625, accuracy: 0.8073576092720032\n",
      "----------------\n",
      "Epoch 36\n",
      "----------------\n",
      "Train loss: 0.529296875\n",
      "----------------\n",
      "Epoch 37\n",
      "----------------\n",
      "Train loss: 0.52978515625\n",
      "----------------\n",
      "Epoch 38\n",
      "----------------\n",
      "Train loss: 0.51708984375\n",
      "----------------\n",
      "Epoch 39\n",
      "----------------\n",
      "Train loss: 0.5205078125\n",
      "----------------\n",
      "Epoch 40\n",
      "----------------\n",
      "Train loss: 0.51220703125\n",
      "Test loss: 0.58056640625, accuracy: 0.8118077516555786\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "new_model, new_optim_state = train_loop(\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    model,\n",
    "    optim_state,\n",
    "    40, 5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c3d7272c1eba356ec9149ec42daf5acdf55d6fdb447aefce6509807a5e73802"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
